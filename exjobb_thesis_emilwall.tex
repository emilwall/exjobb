% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode
% arara: pdflatex: { draft: true }
% arara: biber
% arara: pdflatex: { synctex: true }
% arara: pdflatex: { synctex: true }

\documentclass[11pt]{article}

\usepackage[swedish,english]{babel} % Enables swedish typesetting, needs to be at top of document
\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX
\usepackage{csquotes} % Recommended to be loaded with babel
\usepackage{textcomp} % Suppress unicode char error
\usepackage{enumitem} % resume numbering in enumerations and customize list appearence
\usepackage[bottom = 110pt]{geometry} % to change the page dimensions
\geometry{a4paper} % paper format, could also be placed in documentclass options
\usepackage{float} % Enable placing images in middle of text
\usepackage{caption} % Adjust caption widths
\usepackage{graphicx} % support the \includegraphics command and options
\usepackage{pgfplots} % charts and graphs
\pgfplotsset{compat=1.8} % Avoid running in backwards compatibility mode (unsuitable tick labels; missing features)
\usepackage[parfill]{parskip} % Begin paragraphs with an empty line rather than an indent
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{verbdef} % inline verbatim
\usepackage{listings} % Display code
\usepackage{color} % Used for color in code listings
%\usepackage{titling} % required for setlength droptitle (below)
%\setlength{\droptitle}{-70pt} % Adjust title height
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{} % fancyhdr style reset for header
\lfoot{}\cfoot{\thepage}\rfoot{} % fancyhdr style reset for footer
\usepackage{sectsty} % Section title
\allsectionsfont{\sffamily\mdseries\upshape} % Section font
\usepackage[backend=biber,sorting=none]{biblatex}
\bibliography{references.bib}
\usepackage{url}
\usepackage{hyperref} % href
\usepackage{nameref} % Enable referring to the actual name of the chapter
\usepackage{glossaries}

\makeglossaries

\captionsetup{width=0.8\textwidth}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; requires \usepackage{color} or \usepackage{xcolor}
  basicstyle=\ttfamily\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  %captionpos=t,                    % sets the caption-position to top
  commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,                    % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  language=ML,                 % the language of the code
  morekeywords={*,...},            % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=1,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=4,                       % sets default tabsize to 4 spaces
  %title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}

\title{Rationales and Approaches for Automated Testing of JavaScript and Standard ML}
\author{Emil Wall}
%\date{} % Uncomment to hide date, or provide a date to display

\newglossaryentry{dom}{name={DOM}, description={The Document Object Model (DOM) is a model for the content, structure and style of documents \cite{W3CDOM}. It can be seen as the tree structure of elements that HTML code consists of. A common use of JavaScript is DOM manipulation, which means dynamically changing attributes and style of the elements, or adding and removing (groups of) elements}, first={The Document Object Model (DOM)}, long={The Document Object Model}, short={DOM}}

\newglossaryentry{js}{name={JS}, description={JavaScript (JS) is a scripting language primarily used in web browsers to perform client-side actions not feasible through plain HTML and CSS. It is formally defined in the ECMAScript Language Specification (5.1 version) \cite{ECMAScriptSpecification}, ISO/IEC 16262:2011}, first={JavaScript (JS)}, long={JavaScript}, short={JS}}

\newglossaryentry{sml}{name={SML}, description={Standard ML (SML) is a functional language that performs type inference at compile time and has few mutable state features, used primarily in computer science and mathematical research. SML is formally specified in The Definition of Standard ML \cite{DefinitionStandardML}}, first={Standard ML (SML)}, long={Standard ML}, short={SML}}

\newglossaryentry{testing}{name={Testing}, text={testing}, description={Defined here as automated software testing, unless otherwise specified. Manual testing and most forms of acceptance testing is outside the scope of this thesis}}

\newglossaryentry{tdd}{name={TDD}, description={Test-Driven Development (TDD), see section \ref{subsec:tdd} for a definition}, first={Test-Driven Development (TDD)}, long={Test-Driven Development}, short={TDD}}

\newglossaryentry{bdd}{name={BDD}, description={Behaviour-Driven Development (BDD), see section \ref{subsec:bdd} for a definition}, first={Behaviour-Driven Development (BDD)}, long={Behaviour-Driven Development}, short={BDD}}

\newglossaryentry{cli}{name={CLI}, description={A program with a command-line interface (CLI) is controlled by clients through successive lines of text (commands) input in a console}, first={command-line interface (CLI)}}

\newglossaryentry{gui}{name={GUI}, description={A graphical user interface (GUI) provides, unlike a CLI, visual interactions and feedback for a client}, first={graphical user interface (GUI)}, long={graphical user interface}, short={GUI}}

\newglossaryentry{mooc}{name={MOOC}, first={MOOC (Massive Open Online Course)}, description={``A MOOC [Massive Open Online Course] is an online course with the option of free and open registration, a publicly-shared curriculum, and open-ended outcomes. MOOCs integrate social networking, accessible online resources, and are facilitated by leading practitioners in the field of study. [...] The term came into being in 2008, though versions of very large open online courses were in existence before that time.'' \cite[p.~10]{MOOC}}}

\newglossaryentry{sut}{name={SUT}, description={A system under test (SUT) is code intentionally exercised by a test. A unit under test (UUT) is a SUT that is clearly delimited from the rest of an application}, first={system under test (SUT)}, long={system under test}, short={SUT}}

\newglossaryentry{matcher}{name={Matcher}, text={matcher}, description={BDD terminology for an assertion. Also commonly called expectation}}

\newglossaryentry{spec}{name={Spec (specification)}, text={spec}, first={spec (specification)}, description={BDD terminology for a test}}

\newglossaryentry{dry}{name={DRY}, first={DRY (Don't Repeat Yourself)}, description={The point of the Don't Repeat Yourself (DRY) principle is \emph{not} that the same lines of code cannot occur twice in a project, but that the same \emph{functionality} must not occur twice. Two bits of code may seem to do the same thing while in reality they don't. This applies to eliminating duplication both in production code and tests, do not overdo it since that will harm maintainability rather than improve it. Sometimes it is beneficial to allow for some duplication up front and then refactor once you have a clear picture about how alike the two scenarios actually are, if necessary. \cite[questions~69-70]{Edelstam}}}

\newglossaryentry{yagni}{name={YAGNI}, first={YAGNI (You Ain't Gonna Need It)}, description={The XP (Extreme Programming) principle YAGNI (You Ain't Gonna Need It) is about not introducing anything except if you are entirely sure that it will be needed}}

\newglossaryentry{ci}{name={CI}, description={Continuous Integration (CI) is a practise based on frequently merging new code with a main code repository, commonly using principles such as automated build, testing and deployment}, first={Continuous Integration (CI)}, long={Continuous Integration}, short={CI}}

\newglossaryentry{autodeploy}{name={Automated Deployment}, text={automated deployment}, description={Auto-deploy enables automatic release to test or production, possibly as part of a Continuous deployment strategy}}

\newglossaryentry{testfixture}{name={Test Fixture}, text={test fixture}, description={A test fixture is an environment or a fixed state that is needed for certain tests to run. It can be set up and teared down for each test or set up once and used multiple times, requiring tests to clean up after themselves to avoid problems of shared mutable state but providing a slight increase in speed \cite[question~5]{Rovegard}}}

\begin{document}

\input{frontpages}
%\tableofcontents

\printglossaries

\newpage

\section{Introduction}

Imagine the following scenario: You have been working for many months on a medium size web application project, with demanding technical challenges, using a framework that was previously unknown to you and with constantly changing requirements. People have come and gone from your team and there are parts of the code that no one in your team dares to modify, because no one understands it and too much of the functionality depends on it. Over time, patches and hacks have spread to all over the code base and it feels as if for every bug you fix, new ones are introduced and the complexity of the application is constantly increasing. Every few weeks you feel unbalanced and nervous about the next release, with frightful memories fresh in mind and a feeling that you should be able to do better.

Now envision this: Despite the challenging requirements and conditions, you are relatively sure that the application works as it should. You feel safe in telling the customer when a feature has been implemented because the automated tests indicate that it works and that nothing else has broken. The application has a modular design and you have a good feeling of what every part is supposed to do and how the system works as a whole. This makes it easier to implement change requests and you spend relatively little time debugging, because the tests generally give you precise indications about which parts of the code are affected by your changes. Whenever there is a bug, you capture it with tests so that you will easily notice if it is re-introduced. Releasing a new version is simple and you feel proud of being part of the team.

The main difference between these scenarios is that the second one requires a pervading \gls{testing} effort from the team. In this thesis, obstacles that make \gls{testing} difficult have been investigated. Many of the topics discussed are applicable to any programming language, but it was decided to look at \gls{js} and \gls{sml} specifically because the automated testing community around \gls{js} still has some ground to cover \cite[p.~xix]{Tddjs} and for \gls{sml}, a less known functional language, the situation is even more severe.
\Gls{sml} and \gls{js} both have problems with testing culture, but for different reasons. Client-side \gls{js} testing in particular is a concern shared by many, and until recently there was no \gls{bdd} framework available for \gls{sml}.
The difference in testing efforts between programming languages is evident when comparing \gls{js} to other programming communities such as Ruby and Java. As illustrated by Mark Bates \cite{TestingStatistics}:

\begin{quote}
``Around the beginning of 2012, I gave a presentation for the Boston Ruby Group, in which I asked the crowd of 100 people a few questions. I began, `Who here writes Ruby?' The entire audience raised their hands. Next I asked, `Who tests their Ruby?' Again, everyone raised their hands. `Who writes JavaScript or CoffeeScript?' Once more, 100 hands rose. My final question: `Who tests their JavaScript or CoffeeScript?' A hush fell over the crowd as a mere six hands rose. Of 100 people in that room, 94\% wrote in those languages, but didn't test their code. That number saddened me, but it didn't surprise me.''
\end{quote}

\begin{center}
\begin{tikzpicture}
\begin{axis}[
title={Percentage of crowd testing their code \cite{TestingStatistics} (Boston Ruby Group presentation, 2012):},
xbar,
width=12cm, height=3.5cm, enlarge y limits=0.5,
xlabel={\#participants},
symbolic y coords={JS,Ruby},
ytick=data,
nodes near coords, nodes near coords align={horizontal},
]
\addplot coordinates {(6,JS) (100,Ruby)};
\end{axis}
\end{tikzpicture}
\end{center}

The main goal with this thesis was to investigate why \gls{js} and \gls{sml} testing has been performed to such a small extent historically, describe some benefits and practises of doing it, and identifying problems that are yet to be solved.

\subsection{Motivation}
\label{subsec:motivation}

Why testing, you may ask. Jack Franklin, a young \gls{js} blogger from the UK, gives three reasons: it helps you to plan out your APIs, it allows you to refactor with confidence, and it helps you to discover regression bugs (i.e. when old code breaks because new code has been added). Writing tests to use a library before actually writing the library puts focus on intended usage, leading to a cleaner API. Being able to change and add code without fear of breaking something greatly accelerates productivity, especially for large applications. \cite{JackFranklin} Without tests, the ability to refactor (see section \ref{subsec:refactor}) is greatly hampered and without refactoring, making changes becomes harder over time, the code becomes harder to understand, the number of bugs increases and more time will be spent debugging \cite[p.~47-49]{Refactoring}. In order to avoid this, code should be tested, preferably as an activity integrated with the rest of the development rather than seen as a separate task. Writing tests first ensures testability, which may also imply adherence to principles such as separation of concerns and single responsibility \cite[p.~35-37]{Clean}.

Tests can also serve as documentation and monitoring of the code. Given that the tests are readable and focus on the behaviour of the code, developers can rely on them to understand the production code that they are unfamiliar with or has not worked with for a long time, and measure progress in terms of implementing acceptance tests. There are also benefits for the product management: awareness of how the product performs on different platforms and software environments is reassuring when communicating with customers \cite[question~38]{Ahnve}.

Figure~\ref{fig:distance} (page~\pageref{fig:distance}) can work as an analogy for how testing influences development pace. Developers writing code without tests are like pure sprinter swimmers, they will be fast in the beginning of a project but over time the increasing complexity of the code will force them to go much slower. Developers that writes tests as part of the development process are more like pure distance swimmers, they maintain a sustainable pace. It is arguably slower in the beginning, but productivity does not decrease as drastically over time. The analogy is not perfect -- development pace is typically more dependent on system complexity than swimming speed is on distance, but this on the other hand only serves to further emphasise the point. Testing is a long term investment.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{pics/distance.png}
\caption{Long distance swimmers are initially slower but have better endurance than sprinters.
A software development project displays similar properties, developing with tests is like training for long distances.
If a project will be large enough to reach break even, testing will pay off.
Image courtesy of graph to the left: Paul Newsome at \small \url{www.swimsmooth.com}}
\label{fig:distance}
\end{figure}

Implementations of \gls{js} differ, especially for host objects such as document and XMLHttpRequest whose semantics are not fully defined in the specification but also for native objects that may be missing or behave differently in some implementations because they were not defined in previous specifications. Automated testing can help you discover bugs that appear due to such deviant behaviour in present and future versions. \gls{js} is also particularly important to test due to its dynamic properties \cite{AutomatedTesting} that complicate static analysis (however, there are lint tools, see section~\ref{subsec:refactor}), unwieldy syntax and aberrant object orientation support. \gls{js} is a complex language that requires many work-arounds because of its scoping rules and other unexpected behaviour \cite[appendix A]{GoodParts}. Despite the wide variety of testing frameworks that exists for \gls{js}, it is generally considered that few developers use them and instead rely on manual testing \cite{AutomatedTesting}.

Several implementations of \gls{sml} exist and just as with \gls{js}, there are some differences between them. \gls{sml} has extensive static analysis capabilities and since side effects are relatively rare, the output of functions tends to be predictable, leading to lower complexity in many cases, at the cost of flexibility. \gls{sml} has no built in support for automated testing and there are few testing frameworks available (see section \ref{mltestingframeworks}).

More than 90 \% of today's websites use \gls{js} \cite{BusinessJavascript} and its applications have become increasingly complex \cite[question~23]{Ekelof}. \gls{sml} on the other hand, although not nearly as widespread, is used in critical applications. The potential risk of economic loss associated with untested code being put into production, due to undetected bugs, shortened product lifetime and increased costs in conjunction with further development and maintenance, constitutes the main motivation for this thesis.

\Gls{sml} is a functional language just like \gls{js}, but is not implemented in browsers and has a static type system, lack of (prototype-based) object orientation support, and limited support for mutation. Because of this, \gls{sml} is typically used in education, back-end logic and algorithm design than for web application front-ends. \Gls{sml} is well suited for formal verification, which in theory is excellent, but practical aspects such as how difficult formal verification is to do, how much benefits there are for maintainability, modularity and code design, and the time and resources required to do it need to be considered. In which scenarios is formal verification feasible? How does it affect productivity and the ability to get quick and frequent feedback whether the program is still correct? Even though testing can seldom provide the same guarantees as formal verification regarding correctness, there are many scenarios in which it is more cost effective and makes life easier for the developer. The two can of course also be used in parallel. See section \ref{subsec:formalverification} for more on formal verification.

Unit testing is particularly powerful when in combination with integration tests in a \gls{ci} build with \gls{autodeploy}.
This enables you to harness the power of \gls{ci}, avoiding errors otherwise easily introduced as changes propagate and affect other parts of the system in an unexpected way. The integration tests will make developers aware if they are breaking previous functionality, when changing parts of the system that the \gls{js} depends upon.

This paves the way for test-driven development, which brings benefits in terms of the design becoming more refined, and increased maintainability. To achieve these gains from testing, the tests themselves need to be of high quality: they should be maintainable, fast and test the right thing. How to achieve this is an important part of this thesis, which is discussed in section~\ref{subsec:stateofevents}.

\subsection{Background to Project}
\label{mltestingframeworks}

This thesis was written at Valtech AB, an IT consulting company based in Stockholm, with 185 employees (2013) specialised in digital strategy and full stack web and mobile development. The company has an interest in techniques for testing \gls{js}, but prioritises individual motivation and passion over deciding what employees should write about, which is why \gls{sml} testing is also covered here.

The first known \gls{js} testing framework JsUnit\footnote{\url{https://github.com/pivotal/jsunit}} was created in 2001 by Edward Hieatt \cite{GoingFaster} and since then several other test framework has appeared such as the testing framework for jQuery: QUnit\footnote{\url{http://qunitjs.com/}}, and JsUnits sequel Jasmine\footnote{\url{http://pivotal.github.com/jasmine/}}. There are also tools for mocking such as Sinon.JS\footnote{\url{http://sinonjs.org/}} (see section~\ref{subsec:mocking}). It seems as if the knowledge of how to get started smoothly, how to make the tests stable and time efficient, and what to test, is rare. Setting up the structure needed to write tests is a threshold that most \gls{js} programmers do not overcome \cite{TestingStatistics} and thus, they lose the benefits, both short and long term, otherwise provided by testing.

There is only one production quality testing framework available for \gls{sml}, namely QCheck\footnote{\url{https://github.com/league/qcheck}}. A few other frameworks exist but have not gained any traction and are relatively small, typically less than a year old and not under active development. The recent increase in the number of testing frameworks could be a consequence of developers being more willing to share their work as open source, an increased use of \gls{sml} testing in education (see section~\ref{subsec:tddmooc}), or an indication that testing in general has been increasingly popular over the last couple of years, as can be seen in the \gls{js} community \cite[question~1]{Edelstam}. An exhaustive list of the other \gls{sml} testing frameworks and a short discussion about their differences can be found in the Appendix.

Material on how to test \gls{sml} properly is hard to come by. Similarly, in guides on how to use different \gls{js} testing frameworks, examples are often decoupled from the typical use of \gls{js} -- the Web -- which is a problem \cite[question~3]{Ekelof} although it has become better compared to a couple of years ago \cite[question~27]{Ahnve}. Examples tend to illustrate merely testing of functions without side effects and dependencies. Under these circumstances, the testing is trivial and most \gls{js} programmers would certainly be able to put up a test environment for such simple code.

Examples are useful when learning idiomatic ways of solving problems. Code tends to end up being more complicated than examples you read because it is hard to come up with useful abstractions that make sense. Those who write examples to illustrate a concept always have to find a tradeoff between simplicity, generality and usefulness, and tend to go for simplicity \cite[questions~56-57]{Edelstam}. This can for example be observed in \cite[p.~13-45]{Refactoring} where the tests and their setup are omitted despite their claimed importance. Combining different concepts may help to achieve code with good separation, that can be tested by simple tests.

In contrast to examples often being simple and seldom providing a full picture, the problem domain of this thesis is to focus on how to test the behaviour of \gls{js} that manipulates \gls{dom} elements, fetches data using asynchronous calls, validates forms, communicates through APIs or manipulates the appearance of a web page (see section \ref{sec:jsproblems}). The domain also includes testing of \gls{sml} in general.

\subsection{Scope and Delimitations}

The scope of this thesis is mainly limited to \emph{automated} testing of \gls{sml} and \emph{client side} \gls{js}. This means that the impact of frameworks specialised for server side \gls{js} code such as vows\footnote{\url{http://vowsjs.org/}} and cucumis\footnote{\url{https://github.com/noblesamurai/cucumis}} has not been considered. Testing client side code is by no means more important than server side, but in many aspects it is different and sometimes harder. Reasons for choosing \gls{js} and \gls{sml} over other programming languages have already been covered in the introduction.

\gls{js} testing frameworks that are no longer maintained such as JsUnit\footnote{\url{https://github.com/pivotal/jsunit}} and JSpec\footnote{\url{https://github.com/liblime/jspec}} was also deliberately left out of consideration. Others were left out because of a smaller user base or lack of unique functionality; among these we find TestSwarm, YUI Yeti and RhinoUnit and the majority of the \gls{sml} testing frameworks (see Appendix). These are useful tools but could not be included due to time limitations.

Manual testing was not covered to any significant extent, since it is outside the scope of test-driven development and automated testing. Naturally, there are many situations where manual testing is required, but in this thesis \emph{testing} typically refers to automated testing.

Since \gls{sml} has a smaller user base than \gls{js}, the majority of the research in this thesis has been focused on \gls{js}. Researching today's limited testing of \gls{js} may be done from different perspectives. There are soft aspects such as:
\begin{itemize}[label={--}]
\item Differences in attitudes towards testing between different communities and professional groups and knowledge about testing among \gls{js} developers (section~\ref{sec:testingculture})
\item How \gls{js} is typically conceived as a language and how it is used (section~\ref{ssubsec:interviewees})
\item Economic viability and risk awareness (section~\ref{subsec:projectrisks})
\end{itemize}

There are also more technical aspects:
\begin{itemize}[label={--}]
\item Testability of \gls{js} code written without tests in mind (section~\ref{sec:testability})
\item Usability of testing tools and frameworks (sections \ref{subsec:jstestdriver} and \ref{subsec:frameworks})
\item Limitations in what can be tested (section \ref{sec:testability})
\item Complexity in setting up a test environment; installing frameworks, configuring build server, exposing functions to testing but not to users in production, etc. (section \ref{sec:testability})
\end{itemize}

An important part of the scope has been to account for how to proceed conveniently with \gls{js} and \gls{sml} testing. The ambition was to cover not only the simplest cases but also the most common and the hardest ones, and to introduce available tools and frameworks. Many tutorials for testing frameworks today tend to focus on the simple cases of testing, possibly because making an impression that the framework is simple to use has been more highly prioritised than covering edge cases of how it can be used that might not be relevant to that many anyway. To provide guidance in how to set up a testing environment and how to write the tests, attention was paid to the varying needs of different kinds of applications. It was also important to describe how to write tests that are as maintainable as the system under test, to minimise maintenance costs and maximise gain.

Rather than proposing best practices for \gls{js} testing, the reader should be made aware that different approaches are useful under different circumstances. This applies both to choice of tools and how to organise the tests.

A full evaluation of the most popular testing and application frameworks is not within the scope of this thesis, but others have done it \cite{JackFranklin}\cite{SebastianPorto}. Popular \gls{js} testing frameworks include assertion frameworks such as Jasmine, qUnit, expect.js and chai, and drivers/test runners such as Mocha, JsTestDriver, Karma and Chutzpah\footnote{\url{http://chutzpah.codeplex.com/}} which may have their own assertion framework built in but are typically easy to integrate with other assertion frameworks using adapters or via built-in support.

\subsection{Reading instructions}

This thesis is about difficulties and experiences with \gls{js} and \gls{sml} testing, and has been organised in favour of readers mainly interested in one of these programming languages. It contains a case study on testability aspects of adding tests to an existing \gls{js} application (section~\ref{sec:testability}), problems specific to \gls{js} and \gls{sml} testing (sections~\ref{sec:jsproblems} and~\ref{sec:smlproblems}), considerations from writing and using a \gls{bdd} framework in \gls{sml} (section~\ref{subsec:describesml}) and implications of testing culture that has been researched through interviews (section~\ref{sec:testingculture}). These sections are preceded by an overview of what others have done in the fields of \gls{js} and \gls{sml} testing (section~\ref{sec:previouswork}), the methods that were used for writing this thesis (section~\ref{sec:methods}) and a technical background that explains some of the concepts that appear in the rest of the thesis (section~\ref{sec:technicalbackground}). The final section is conclusions (section~\ref{sec:conclusions}), including summaries and proposals of future work.

Readers experienced with or uninterested in testing concepts and web application development may skip the technical background (section~\ref{sec:technicalbackground}). Readers interested mainly in \gls{js} testing may skip sections~\ref{subsec:previousworksml}, \ref{sec:smlproblems} and~\ref{subsubsec:whynotsml}. Readers interested mainly in \gls{sml} testing may skip sections~\ref{subsec:previousworkjs}, \ref{subsec:browserautomation}, \ref{sec:testability}, \ref{sec:jsproblems} and~\ref{subsubsec:whynotjs}.

The glossary, which is located before this introduction, contains definitions of abbreviations and technical terms used throughout the text.

\section{Previous work}
\label{sec:previouswork}

This section contains an overview of what others have done within the fields of \gls{js} and \gls{sml} testing.

\subsection{JavaScript testing}
\label{subsec:previousworkjs}

In 2010, a student at the Swedish royal institute of technology called Jens Neubeck wrote a master thesis about test-driven \gls{js} application development \cite{Neubeck}. In his thesis, he evaluated Crosscheck, HtmlUnit och Selenium, with the conclusion that none of them were mature enough to be used for the considered applications. Today, HtmlUnit and Selenium have evolved and there are new tools available such as PhantomJS, Buster.JS and Karma, so the conclusion might not hold anymore. Tools such as JsTestDriver and Jasmine were not considered and the results were based purely on original work, with no \gls{js} specific academic sources, so it has not been possible to build upon his findings here.

The main source of reference within the field of \gls{js} testing today is \emph{Test-Driven JavaScript Development} \cite{Tddjs} by Christian Johansen, which deals with \gls{js} testing from a \gls{tdd} perspective. Johansen is the creator of Sinon.JS\footnote{\url{http://sinonjs.org/}} and a contributor to a number of testing frameworks hosted in the open source community. The book takes a rather practical approach to \gls{js} testing by explaining many aspects of how \gls{js} works and by including practise exercises. It is not very scientific but makes up for this with its pragmatism and roots in the software industry.

Today blog posts and books about \gls{js} are in abundance and examples can often be found in the documentation of frameworks. When it comes to examples of testing in general, there are several classics to refer to \cite{KentBeck}\cite{TestPatterns}. For examples of \gls{js} testing specifically the alternatives have been scarce historically, but recently a large number of books about \gls{js} testing has been published. \emph{JavaScript Testing, Beginner's Guide} \cite{JSBeginners} is an introductory book about \gls{js} that covers some aspects of testing, \emph{JavaScript Testing with Jasmine} \cite{JasmineBook} covers the Jasmine testing framework in detail, \emph{Behaviour Driven Development with JavaScript} \cite{BDDJS} presents a \gls{bdd} perspective of \gls{js} testing, \emph{JavaScript Unit Testing} \cite{JSUT} looks at the assertions and asynchronous testing capabilities of Jasmine, YUI Test, QUnit and JsTestDriver, \emph{Using Node.js for UI Testing} \cite{UsingNode} covers ways of automating testing of web applications with Zombie.js and Mocha, and \emph{Testable JavaScript} \cite{TestableJS} looks at ways of reducing complexity of \gls{js} code and discusses principles and tools (mainly YUI Test) for \gls{js} testing and maintainability in general. All of these were published this year (2013), except \emph{JavaScript Testing, Beginner's Guide} which was published the same year as Johansen's book, in 2010.

There are many academic articles about testing web applications available, and quite a few of them focus on \gls{js} specifically \cite{AutomatedTesting}\cite{ContractTesting}\cite{AutomatedAcceptance}\cite{Wild}\cite{DOMJavascript}. There is also a lot of material on testing patterns and how to write concise, useful and maintainable tests \cite[part~III]{TestPatterns}\cite[ch.~3-5]{BDDJS}\cite[p.~461-474]{Tddjs}\cite[p.~86-87]{TestableJS}\cite[p.~13-14]{JasmineBook}.

\emph{A Framework for Automated Testing of JavaScript Web Applications} \cite{AutomatedTesting} focus on automatically generating tests to achieve a high degree of code coverage. The problem with this is that the ability to employ test driven development is generally more valuable than high code coverage, due to its effect on the system design (see section~\ref{subsec:coveragecriteria} for further discussion). Automatically generated tests can be harder to maintain and will tend to fail for unintended reasons as the code changes, unless the tests are re-generated.

\emph{Automated Acceptance Testing of JavaScript Web Applications} \cite{AutomatedAcceptance} describes a way to specify intended behaviour of a web application and use a web crawler to verify the expectations. It appears as a promising alternative to using Cucumber in conjunction with Selenium (see section~\ref{subsec:browserautomation}), but more case studies are needed in order to evaluate its usefulness, applicability and scalability. %TODO describe Cucumber

Sebastien Salva and Patrice Laurencot has described how STS automata can be applied to describe asynchronous \gls{js} applications and generate test cases \cite{AutomatedAjax}.

Heidegger et al. cover unit testing of \gls{js} that manipulates the \gls{dom} of a web page using techniques from software transactional memory (STM) to restore test fixtures \cite{DOMJavascript}. Ocariza et al. have investigated frequency of bugs in live web pages and applications \cite{Wild}. These are both aimed at testing client side \gls{js} that runs as part of web sites.

Phillip Heidegger and Peter Thiemann has addressed the issue of type related errors in \gls{js} by introducing JSConTest, a contract framework that enables guided random testing by specifying types and relations of the arguments and return value of functions \cite{ContractTesting}.

\subsection{Standard ML testing}
\label{subsec:previousworksml}

Except for discussions on how to perform mathematical proofs and equality tests of polymorphic types, there are no books that cover testing of \gls{sml}. The main sources of reference for \gls{sml} are \emph{The Definition of Standard ML} \cite{DefinitionStandardML} which covers the language syntax and semantics, \emph{The Standard ML Basis Manual} \cite{BasisManual} which describes the standard library, \emph{Elements of ML Programming} \cite{ElementsML} which cover the features of \gls{sml} in a more comprehensible fashion, \emph{ML for the Working Programmer} \cite{WorkingProgrammer} which is somewhat outdated but comprehensive, and various lecture notes \cite{ProgSml97}\cite{ProgSmlHarper}\cite{FunctionalML}\cite{NotesSMLNJ}. None of these describe ways of doing automated testing and there seems to be an attitude against testing based on that it can not prove absence of errors in the way formal verification can \cite[p.~16]{ProgSmlHarper}.

Articles on testing of \gls{sml} are hard to find, none cover \gls{sml} testing within a web applications context and most are concerned with formal verification or related to the QuickCheck tool which can be used to generate tests for Haskell code, using \gls{sml} as an intermediate language. However, the ideas of QuickCheck has been directly applied to \gls{sml} in the QCheck testing framework, which is covered in \emph{Random Testing of ML Programs} \cite{RandomML}, a master thesis from 2010. In QCheck, tests are specified in terms of properties rather than generated from the code itself or based on user interaction data. While avoiding the circularity of generating tests based on the code that should be tested, this approach instead suffers from the difficulty of identifying and expressing properties that should hold, and there may be uncertainty in how well the properties are actually tested.


\section{Methods}
\label{sec:methods}

The methods used in this thesis comprised situational analysis, interviews and programming activities in \gls{js} and \gls{sml}. The work of this thesis began with an extensive literature study and an overview of technologies and frameworks. Interviews of \gls{js} developers of different background were performed and analysed. There was also some hands on evaluation of tools and frameworks, assessment of testability and impact of adding tests to existing projects, and a small testing framework was developed in \gls{sml} and used in a \gls{mooc}.

\subsection{Literature Study}

As mentioned in \nameref{sec:previouswork} (section \ref{sec:previouswork}), several new titles were published while writing this thesis, so the literature study continued to the very end. The books, articles and internet sources served both as reference to complement the interview material and as starting point for many of the experimental testing activities that were carried out (see next subsection).

Since there was an abundance of material on \gls{js} testing, a full review was not viable. For \gls{sml} testing on the other hand, there were virtually no material on \gls{sml} testing available, so any claims had to be based on practical observations and comparisons with formal verification instead. Since many relevant facts and principles hold for more programming languages than just these two, some classical works within the field of testing were included in the study as well.

\subsection{Programming}
\label{subsec:programming}

In order to describe ways of writing tests for \gls{js}, the practical work involved adding tests to an existing \gls{js} application (see section \ref{subsec:asteriods}), performing \gls{tdd} exercises from Test-driven JavaScript Development \cite[part~III]{Tddjs} and doing some small \gls{tdd} projects during the framework evaluation. There were plans to have a workshop field study, where programmers would work in pairs to solve pre-defined problems using \gls{tdd}, but in the end it was decided that it would be too difficult to extract useful data from such an activity.

The development of an \gls{sml} testing framework (see section \ref{subsec:describesml}) and using the developed framework in practise to solve programming assignments in a \gls{mooc}, namely the Coursera Programming Languages course (see section~\ref{subsec:tddmooc}), was also a substantial part of the work effort in this thesis.

A thorough evaluation of frameworks for \gls{js} and \gls{sml} was not part of the scope, but since they pose a significant part of how to solve problems with testing many were involved anyway. The following \gls{js} testing frameworks have been part of the practical work:
Jasmine\footnote{\url{http://pivotal.github.com/jasmine/}},
qUnit\footnote{\url{http://qunitjs.com/}},
Karma\footnote{\url{http://karma-runner.github.io/}},
Mocha\footnote{\url{http://visionmedia.github.com/mocha/}},
JsTestDriver\footnote{\url{https://code.google.com/p/js-test-driver/}},
Buster.JS\footnote{\url{http://docs.busterjs.org/}} and
Sinon.JS\footnote{\url{http://sinonjs.org/}}. Apart from the \gls{mooc} programming assignments, all code is publicly available on my Github account \emph{emilwall}, together with the \LaTeX~code for this report.

\subsection{Interviews}

The \gls{js} community is undergoing more rapid changes than the \gls{sml} community, so interviews were focused on \gls{js} to obtain up-to-date information about how it is currently used. They were first and foremost qualitative in nature, carried out as semi-structured case studies in order to prioritise insight into the problem domain and gather unique views and common experiences, which might not be picked up in a standardised survey or other efforts to quantitative research methods. The interviews were between 20 and 60 minutes long and were conducted both in person and via Internet video calls.

\subsubsection{Interview Considerations}

The preparations before the interviews included specifying purpose and which subjects to include, select interviewees, preparing questions and adjust the material to fit each interviewee. The chance of finding the true difficulties of \gls{js} testing was expected to increase with open questions. The interviews took place once preliminary results and insights from the literature study could be used as basis for the discussions.

The purpose of the interviews was to investigate attitudes and to get a reality check on ideas that had emerged during previous work. Selecting the interviewees was to a large extent done based on availability, but care was also taken to include people outside of Valtech and to get opinions from people with different background (front-end, back-end, senior, junior, etc.). There were five interviews and some email conversation, which can all be found in the Appendix.

The interviews were performed in Swedish to allow for a more fluent conversation and minimise risk of misunderstandings. They were transcribed (see Appendix), each question was given a number, and the most relevant parts were translated and included in this report with reference to the question numbers. The interviewees were asked prior to the interviews if it was ok to record the conversation and if they wanted to be anonymous, everyone agreed to be recorded and mentioned by name.

The interviewees received questions beforehand via mail which most of them answered before the interview. This allowed the interviews to focus on the vital parts rather than personal background and opinions about \gls{js}. The questions that were sent out before the interviews were mainly about previous experience with \gls{js} and testing, frameworks, attitudes towards the language, difficulties with testing and opinions and observations on benefits of testing.

\subsubsection{The Interviewees}
\label{ssubsec:interviewees}

\begin{enumerate}
\item Johannes Edelstam, an experienced Ruby and \gls{js} developer, organiser of the sthlm.js meet-up group, a helping hack, and a former employee of Valtech, now working at Tink. He has a positive attitude towards \gls{js} as a programming language and has extensive experience of test driven development.

\item Patrik Stenmark, a Ruby and \gls{js} developer since 2007. He is also an organiser of a helping hack and a current employee at Valtech. He considers \gls{js} to be inconsistent and weird in some aspects but appreciates the fact that it is available in browsers and has developed large single page applications (SPA) in it.

\item Marcus Ahnve, a senior developer and agile coach who has been in business since 1996 working for IBM, Sun Microsystems and ThoughtWorks, and as CTO for Lecando and WeMind. He is currently working at Valtech. He is an experienced speaker and a founder of Agile Sweden, an annual conference since 2008. He is also experienced with test driven development in Java, Ruby and \gls{js}.

\item Per Rovegård, a developer with a Ph.D. in Software Engineering from Blekinge Institute of Technology. He has worked for Ericsson and is currently a consultant at factor10 where he has spent the last year developing an AngularJS application, with over 3000 tests. He is the author of the programatically speaking blog and has given several talks at conferences and meet-ups, most recently about Angular and \gls{tdd} at sthlm.js on the 2nd of Oct 2013 but the interviews took place in August over Skype.

\item Henrik Ekelöf, a front-end developer who has seven years of professional experience with \gls{js}. He has previously worked as webmaster and web developer at Statistics Sweden and SIX and is now technical consultant at Valtech. I met him in person during my introduction programme in Valtech where he had a session with us about idiomatic \gls{js}, linting and optimisations, but this interview was done over Skype since he works out of town.
\end{enumerate}

\subsubsection{Further contacts}

As can be seen at the end of the Appendix, there were some additional email conversations. Among those were: Fredrik Wendt, a senior developer and consultant at Squeed specialising in team coaching with coding dojos, \gls{tdd} and agile methodologies. David Waller, teacher at Linnéuniversitetet in a course about Rich Internet Applications with JavaScript. Marcus Bendtsen, teacher at Linköpings Universitet in a course about Web Programming and Interactivity.


\section{Technical Background}
\label{sec:technicalbackground}

This section gives an overview of concepts and tools relevant to understanding this thesis. Readers with significant prior knowledge about web development and \gls{js} testing may skip this section.

\subsection{Basics of Testing}
\label{subsec:testingbasics}

Every developer performs testing in one way or another. Running an application, interacting with it and observing the results is one form of testing. The more time spent on developing the application, the more evident the need for automated tests tends to become, to reduce the amount of manual work and time spent repeating the same procedures.

Automated testing is commonly divided into different categories of tests, that exercise the code in different ways and for different reasons. Unit testing and integration testing is perhaps the most common concepts. Unit testing focuses on testing units (parts) of an application, in isolation from the rest of the application, whereas integration testing is about testing that the units fit together. Sometimes there is an overlap between the concepts, where a unit is relatively large.

Commonly, unit tests are more low level and integration tests are more like user interactions. This does not have to be the case however, unit tests can be written in a business value focused fashion and integration tests may be written before there is even a \gls{gui} to interact with \cite[question~20]{Ahnve}. Thinking in terms of APIs, there is usually a possibility of defining what each part should do, so you can write tests in an outside-in fashion, which reduces the risk of developing something that eventually turns out to be redundant or misdirected (remember, \gls{yagni}) \cite[question~29]{Ahnve}. The downside of this approach is that the code required for such a high level test to pass might be complex, so intermediate tests need to be written. Such a test, that is in a failing state for an extended period of time, is sometimes called a pending test \cite[question~31]{Ahnve}. Pending tests can be useful to guide the development in a clear direction, but the number of pending tests should be kept to a minimum, or else they will become outdated. This can be seen as a consequence of such a test not being timely, thereby breaking the last component of the F.I.R.S.T. principle \cite[p.~132-133]{Clean}.

There are potentially both good and bad consequences of testing, both from a short and from a long term perspective. A disadvantage is that setting up the test environment and writing the tests take time. If the testing process is not carried out properly, maintaining the tests can cause frustration. The advantages are that if time is spent thinking about and writing tests, the development of production code will require less effort. Testing provides shorter feedback loops, executable documentation and new ways of communicating requirements with customers. The quality and maintainability of the end result is likely to be positively affected and making changes becomes easier, so ideally, the pace of development does not stagnate. The extra time required to set up the test environment and write the actual tests may or may not turn out to pay off, depending on how the application will be used and maintained.

\subsection{Test-Driven Development}
\label{subsec:tdd}

\acrfull{tdd} is ``an iterative development process in which each iteration starts by writing a test that forms a part of the specification we are implementing'' \cite[p.~21]{Tddjs}.

\gls{tdd} shifts focus from implementations to testing, thereby enforcing a thought process of how to translate requirements into tests. When using \gls{tdd}, the most common reason for a bug is because the \gls{tdd} practitioner has moved on prematurely, i.e. not writing enough tests to find a scenario in which the code does not behave as it should or not fully understood the requirements. These kinds of bug would probably persist regardless of if \gls{tdd} is used or not, but the thing to be careful about here is that there is a risk of not putting as much energy into writing flawless production code when using \gls{tdd}, instead relying on iterative improvement and refactoring. % Move or add to conclusions?

A common principle of \gls{tdd}, especially when carried out in a \gls{ci} build, is that the application should frequently be brought to a state where all tests pass \cite[question~2]{Rovegard}. An exception from this rule is pending tests, but as mentioned in section~\ref{subsec:testingbasics}, these should be kept to a minimum.

\subsection{Behaviour-Driven Development}
\label{subsec:bdd}

\acrfull{bdd} is about describing expected behaviour of systems and writing tests from the outside in. It replaces and adds terminology traditionally used within \gls{tdd} and encourages descriptive naming of tests (or \glspl{spec}) that helps readability and to make failure messages more helpful. \cite[questions~17-18]{Ahnve}

An advantage with \gls{bdd} is how it encapsulates ideas about how tests can be organised, for instance through the Given, When, Then (GWT) form (covered in greater detail in section \ref{subsec:frameworks}). In todays \gls{bdd} frameworks there is often a possibility to separate the basic functionality from the special cases by organising \glspl{spec} in nested describes. This can provide an overview of what an application does just by looking at the \gls{spec} output and is commonly seen in open source projects \cite[question~42]{Stenmark}. Providing a context for \glspl{spec} in this way can help to avoid having a single hard-to-read setup for all tests of a class, which is otherwise common in classical \gls{tdd} and testing in general. Having a single setup can be problematic not only for readability reasons, but also because it creates dependencies between tests. A common alternative to the GWT form is nested describe clauses with it-specs as in Jasmine and \gls{bdd} style Mocha. This requires more discipline from the developer because the context has to be covered by the describe text. The classical \gls{bdd} framework RSpec has a \texttt{when} directive that serves as a compromise between the two styles. \cite[question~19]{Ahnve}

Strictly speaking, you do not need a \gls{bdd} framework to perform \gls{bdd}. J. B. Rainsberger, the author of JUnit Recipes: Practical Methods for Programmer Testing, explained how to do \gls{bdd} in jUnit at the XP 2011 conference in Madrid. The key to doing this is to divide the tests based on system behaviour rather than classes. This is the same concept as when writing \glspl{spec} in Cucumber, another Ruby GWT framework, and the same principle applies to \gls{bdd} in \gls{js} (note that Cucumber can also be used to generate acceptance tests for \gls{js}). This is desirable because it helps you to prioritise the parts that truly matters from a business value point of view over implementation details. \cite[question~20]{Ahnve}

\subsection{Spikes in TDD}
\label{subsec:spikes}

Although writing tests first is a recommended approach in most situations, there is a technique for when you need to try something out before you write tests for it, without compromising testability. Dan North, the originator of \gls{bdd}, came up with a name for this technique: spiking \cite{TwitterDanNorth}. The idea is that you create a new branch in your version control repository, and hack away. Add anything you think might solve your problem, don't care about maintainability, testability or anything of the sort. If you find yourself not knowing how to proceed, discard all changes in the branch and start over. As soon as you get an idea about how a solution could look like you switch back to the previous branch and start coding in a test first fashion. \cite[question~59]{Edelstam}

There is an ongoing discussion about whether or not you have to start over after a spike. Liz Keogh, a well known consultant and core member of the \gls{bdd} community, has published posts about the subject in her blog, in which she argues that an experienced developer can benefit from trying things out without tests (spiking) and then stabilising (refactoring and adding tests) once sufficient feedback has been obtained to reduce the uncertainty that led to the need for spiking \cite{Liz1}. She argues that this allows her to get faster feedback and be more agile without compromising the end result in any noticeable way. In another post, she emphasises that this approach is only suitable for developers who are really good at \gls{tdd}, while at the same time claiming that it is more important to be ``able to tidy up the code'' than ``getting it right in the first place'' \cite{Liz2}. It may seem like an elitist point of view and a sacrilege towards \gls{tdd} principles but in the end, whatever makes you most productive and produces the most valuable software has raison d'être.

Counterintuitive it may seem, throwing away a prototype and starting from scratch to test drive the same feature can improve efficiency in the long run. The hard part of coding is not typing, it is learning and problem solving. A spike \emph{should} be short and incomplete, its main purpose is to help you get your mind set on what tests you can write and what the main points in a solution would be. \cite[question~60]{Edelstam}

A similar concept to Spike and Stabilise is to write markup without tests until a feature is needed that could use an API call. Then you write one or several acceptance tests for it, and can start to work with that feature in a \gls{tdd} fashion. \cite[question~30]{Ahnve} %Likheten är inte perfekt, kanske bör tillägga att det inte finns något stabilise-steg i den approachen.
This way of thinking can lead to a useful rationale regarding when to test -- add tests whenever you introduce an external dependency to make sure that dependency is called correctly under certain circumstances, then if that dependency is something that already exists you can just add it, otherwise you can develop it in a test-driven fashion.

\subsection{Refactoring and Lint-like tools}
\label{subsec:refactor}

Refactoring is ``a change made to the internal structure of software to make it easier to understand and cheaper to modify without changing its observable behaviour'', or the act of applying such changes \cite[p.~46]{Refactoring}. As previously mentioned (section~\ref{subsec:motivation}, \nameref{subsec:motivation}), refactoring prevents program decay and helps to maintain productivity. Just like testing, refactoring is no magic bullet that solves all problems and it is important to know when to do it and when it might not be worth the effort. Suitable situations to refactor are when doing something similar for the third time (a pragmatic approach to the \gls{dry} principle), when it helps to understand a piece of code, when it facilitates addition of a new feature, when searching for a bug and when discussing the code with others, e.g. in code reviews \cite[p.~49-51]{Refactoring}.

There is a serious risk involved in refactoring untested code\cite[p.~17]{Refactoring}, since manually checking that the refactoring does not introduce bugs is time consuming and difficult to do well. However, leaving the code untested means even greater risk of bugs and the refactoring may be necessary in the future anyway, in which case it will be even harder and more error-prone. This problem can be avoided by writing tests first.

A lint-like tool uses static analysis to detect syntax errors, risky programming styles and failure to comply to coding conventions. The use of lint-like tools can be beneficial when refactoring to avoid introducing errors, although it can not fully compensate for lack of tests. There are lint tools available for \gls{js} such as JSLint, JSHint, JavaScript Lint, JSure, the Closure compiler and PHP CodeSniffer. JSLint does provide some help to avoid common programming mistakes, but does not perform flow analysis \cite{JSLint} and type checking as a fully featured compiler would do, rendering proper testing routines the appropriate measure against programming mistakes. There is at least one lint tool available for \gls{sml}, namely SML-Lint\footnote{\url{https://github.com/nrnrnr/SML-Lint}}, but since \gls{sml} is statically typed the need for such a tool is not as great.

Apart from lint-like tools, there are also tools that can be very helpful to see which parts of the code are in most need of refactoring, and to automate certain refactoring actions by facilitating renaming, method extraction, etc. \cite[ch.~5]{Legacy}. Refactoring tools help you identify sections of the code with high cyclomatic complexity, too many methods or with methods doing too many things (or having too many arguments). Relying on metrics such as lines of code is of course not always appropriate due to different coding styles, but at least it provides an overall picture \cite{Kratko}. There is some support for refactoring \gls{js} in IDEs such as Visual Studio (using
JSLint.VS\footnote{\url{http://jslint.codeplex.com/}},
ReSharper\footnote{\url{http://www.jetbrains.com/resharper/}} and/or
CodeRush\footnote{\url{http://www.devexpress.com/Products/CodeRush/}}),
WebStorm IDE\footnote{\url{http://www.jetbrains.com/webstorm/features}} (or
IntelliJ Idea\footnote{\url{http://www.jetbrains.com/editors/javascript_editor.jsp?ide=idea}} using a plugin) and
NetBeans\footnote{\url{https://netbeans.org/kb/docs/ide/javascript-editor.html}}. There are also standalone statistical tools for \gls{js} such as
JSComplexity.org\footnote{\url{https://github.com/philbooth/complexity-report}},
kratko.js\footnote{\url{https://github.com/kangax/kratko.js}} and
jsmeter\footnote{\url{https://code.google.com/p/jsmeter/}} (not to be confused with the
Microsoft Research project\footnote{\url{http://research.microsoft.com/en-us/projects/jsmeter/}}), and general source code analysis software that support \gls{js} such as
Understand\footnote{\url{http://www.scitools.com}},
SonarQube\footnote{\url{http://docs.codehaus.org/display/SONAR/Plugin+Library}} and
Yasca\footnote{\url{http://www.scovetta.com/yasca.html}}.
There seems to be no refactoring tool for \gls{sml} widely available.

\subsection{Mocking and Stubbing}
\label{subsec:mocking}

Mocking and stubbing involves simulation of behaviour of real objects in order to isolate the system under test from external dependencies. This is typically done in order to improve error localisation and execution time, and to avoid unwanted side-effects and dependencies such as communication with databases, across networks or with a file system \cite[ch.~2]{Legacy}. A mock is different from a stub in that it has pre-programmed expectations and built-in behaviour verification \cite[p.~453]{Tddjs}.

In \gls{js}, tools for stubbing can be superfluous because you are able to manually replace functions with custom anonymous functions, that can have attributes for call assertion purposes. The stubbed functions can be stored in local variables in the tests and restored during teardown. This is what some refer to as VanillaJS \cite[question~53]{Edelstam}. It might come across as manual work that you can avoid by using a stubbing tool, but the benefits include fewer dependencies and sometimes more readable code, as mentioned in section~\ref{LiteralFakes} \cite[questions~54-55]{Edelstam}. However, bear in mind that since \gls{js} has no notion of interfaces, it is easy to make the mistake of using the wrong method name or argument order when stubbing a function manually \cite[p.~471]{Tddjs}.

Typical cases for using a stubbing or mocking framework rather than VanillaJS include when your assertion framework has support for it, as is the case for Jasmine, when you feel the need to do a complex call assertion, mock a large API or state expectations up front as is done with mocks. Bear in mind that overly complex stubbing needs can be a symptom for that the code is in need of refactoring \cite[question~34]{Stenmark}, and strive for consistency by using a single method for stubbing -- mixing VanillaJS with Jasmine spies and Sinon.JS stubs will make the tests harder to understand.

In \gls{sml}, stubbing and mocking can be problematic because of its immutability and lexical scoping. There are situations where replacing a definition with another is technically possible, such as if a dependency is located in another file (then you could import a fake version of the definitions in that file instead) or in the rare case where a mutable reference is used, but in most practical applications there is currently no way of stubbing or mocking in \gls{sml}. Perhaps it would be possible to modify an \gls{sml} implementation to allow it, or do something rash such as allowing tests to temporarily modify the source code containing the \gls{sut}, but that is somewhat far-fetched and error prone.

\subsection{Browser Automation}
\label{subsec:browserautomation}

Repetitive manual navigation of a web site is generally boring and time consuming. There are situations where manual testing is the right thing to do, such as when there is no need for regression testing or the functionality is too complicated to interact with for automated tests to be possible (but then the design should probably be improved). Most of the time, tasks can be automated. There are several tools available for automating a web browser: the popular open source Selenium WebDriver, the versatile but proprietary and windows specific TestComplete and Ranorex, the Ruby library Watir and its .NET counterpart WatiN, and others such as Sahi and Windmill.

Selenium WebDriver is a collection of language specific bindings to drive a browser, which includes an implementation of the W3C WebDriver specification. It is based on Selenium RC, which is a deprecated technology for controlling browsers using a remote control server. A common way of using Selenium WebDriver is for user interface and integration testing, by instantiating a browser specific driver, using it to navigate to a page, interacting with it using element selectors, key events and clicks, and then inspecting the result through assertions. These actions can be performed in common unit testing frameworks in Java, C\nolinebreak\hspace{-.05em}\raisebox{.3ex}{\scriptsize\bf \#}, Ruby and Python through library support that uses the Selenium Webdriver API. \cite{Selenium}\cite{Selenium2}

There is also a Firefox plugin called Selenium IDE, that allows the user to record interactions and generate code for them that can be used to repeat the procedure or as a starting point in tests. In the remaining parts of this thesis, we will mean Selenium WebDriver when we say Selenium, and refer to Selenium IDE by its full name.

\subsection{Build tools}
\label{subsec:build}

Build programs play an important role in automating testing, and they are often integrated with version control systems. There exists some general build tools that can be used for any programming language, these are often installed on build servers and integrated with version control systems. Examples include Jenkins, which is often configured and controlled through its web interface although it also has a \gls{cli}, and GNU Make, which is typically configured using makefiles and controlled through \gls{cli}. In addition to these, there are also language specific tools: Ruby has Rake, Java has Maven, Gradle and Ant, C\nolinebreak\hspace{-.05em}\raisebox{.3ex}{\scriptsize\bf \#} has MSBuild and NAnt.

Naturally, there are build tools designed specifically for \gls{js} as well, Grunt\footnote{\url{http://gruntjs.com/}} being the most popular, which can be installed as a node.js package, has plugins for common tasks such as lint, testing and minification, and can be invoked through \gls{cli}. \cite[question~52]{Edelstam} Jake and Mimosa are other well known and maintained alternatives. It is also possible to use Rake, Ant or similar. Just as JsTestDriver and Mocha have adapters for Karma and Jasmine (see section~\ref{subsec:frameworks}), Rake has evergreen\footnote{\url{https://github.com/jnicklas/evergreen}} that allows it to run Jasmine unit tests. \cite{BuildTools}\cite[question~6]{Ahnve}

\Gls{sml} has several compilers such as mosmlc and the MLton whole-program optimizer and a large number of interpreters, but they typically have to be manually integrated with other build tools such as GNU Make.


\section{Testability -- Real world experiences}
\label{sec:testability}

Adding tests for code that was written without testing in mind is challenging \cite[p.~18]{Tddjs}. In this section a case study for doing so in \gls{js} is described, in order to highlight problems and how some of them can be solved. After a short description of how the project was selected and set up, observations regarding the code and the tools used to test it are highlighted, followed by a more general discussion on what to think of when testing an existing application.

\subsection{The asteroids HTML5 canvas game}
\label{subsec:asteriods}

The first step in the case study of adding tests in retrospect was selecting a suitable application. The choice became an HTML5 canvas game, namely asteroids (see figure~\ref{fig:game3}), based on that it combined graphical elements with relatively complex logic and a bit of jQuery, making it a reasonable representative for the typical JavaScript application. Most of the code was reasonably well modularised already from the start and it was easy to get started by simply cloning the repository and opening the supplied HTML file with the canvas element and script locations already defined.

\begin{figure}[ht!]
\centering
\includegraphics[width=1.0\textwidth]{pics/game3.png}
\caption{The asteroids HTML5 canvas game: here the player ship is about to collide with an enemy ship which will destroy them both (unlike what happens when asteroids collide, an example of desirable behaviour to test for)}
\label{fig:game3}
\end{figure}

\subsubsection{Getting Started}
\label{ssubsec:gettingstarted}

The first action was to strip away any redundant files such as iPad specific code and sound files that were not used anyway. This led to a project structure of only game.js, index.html, the licence file, jquery 1.4.1 and a \gls{js} file containing the fonts (used to display score and messages in the game, see figures~\ref{fig:game6} and~\ref{fig:game10}). The next step was to split game.js into separate files in order to test each ``class'' in isolation. This could have been done the other way around, guiding the structure by writing tests first, but it felt natural to work with the source in order to understand it. Refactoring without tests is typically not a good idea \cite[p.~17]{Refactoring}, but it turned out reasonably easy to change the code even without tests as long as care was taken to preserve the order in which the code was executed when moving things to separate files. Since the exercises in \emph{Test-Driven JavaScript Development} \cite{Tddjs} had introduced JsTestDriver, using that with a Jasmine plugin felt like a natural choice at the time, although it involved some issues with syncing the tests in the browser and find the correct versions for the Jasmine adapter to work. The project structure is displayed in figure~\ref{fig:tree}.

\begin{figure}[ht!]
\centering
\includegraphics[width=1.0\textwidth]{pics/tree.png}
\caption{Project structure after splitting \texttt{game.js} into several files and adding JSTD and Jasmine, but before adding any specs}
\label{fig:tree}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{pics/game6.png}
\caption{After a game over screen has been displayed for one second, the user sees this screen with score of previous game and the ability to start a new game}
\label{fig:game6}
\end{figure}

Later on, tests were added in new directories \texttt{sprites-spec} and \texttt{core-spec}, Sinon.js was added to the \texttt{lib} directory for better stubbing, rendering.js was extracted from main.js to enable more selective execution and a reset.js file was added to the \texttt{core} directory, containing just a single line:

\begin{verbatim}
var asteroids = {}
\end{verbatim}

This line ensured that any old definitions in the asteroids namespace were overwritten before reading the files anew. This should not really be necessary, but was done in order to avoid emptying the cache in between each test run with JsTestDriver. An updated version of the application can be found at \url{https://github.com/emilwall/HTML5-Asteroids}.

\subsubsection{Attempts and Observations}
\label{ssubsec:attemptsandobservations}

Once starting to write tests, the first problems were that some of the code was contained in a jQuery context and that the canvas element was not available in the unit testing environment:

\begin{lstlisting}[language=C++, keywords={function, var, new, if, else, switch, case, break, return, for}]
$(function () {
  var canvas = $("#canvas");
  Game.canvasWidth  = canvas.width();
  Game.canvasHeight = canvas.height();

  var context = canvas[0].getContext("2d");

  ... // Omitted for brevity

  window.requestAnimFrame = (function () {
    return  window.requestAnimationFrame       ||
            window.webkitRequestAnimationFrame ||
            window.mozRequestAnimationFrame    ||
            window.oRequestAnimationFrame      ||
            window.msRequestAnimationFrame     ||
            function (/* function */ callback, /* DOMElement */ element) {
              window.setTimeout(callback, 1000 / 60);
            };
  })();

  var mainLoop = function () {
    context.clearRect(0, 0, Game.canvasWidth, Game.canvasHeight);

    ... // Omitted for brevity

  };

  mainLoop();

  $(window).keydown(function (e) {
    switch (KEY_CODES[e.keyCode]) {
      case 'f': // show framerate
        showFramerate = !showFramerate;
        break;
      case 'p': // pause
        paused = !paused;
        if (!paused) {
          // start up again
          lastFrame = Date.now();
          mainLoop();
        }
        break;
      case 'm': // mute
        SFX.muted = !SFX.muted;
        break;
    }
  });
});
\end{lstlisting}

Efforts to load the contained code using ajax were of no gain, so the solution instead came to involve exposing the contents of the context as a separate class \texttt{rendering.js}, and manually inject the canvas and other dependencies into that class. This required turning local variables such as the 2d-context into attributes of the class, to make them accessible from the original location of the code (the jQuery context). This change introduced a bug that manifested itself only when a feature to draw grid boundaries (see figure~\ref{fig:game8}) was activated. The feature was not important and could have been removed, but solutions were considered anyway for investigation purposes. The bug could be fixed by making a couple of variables in the rendering class globally accessible as attributes, but that would break certain abstractions, possibly making the code harder to maintain. A better solution, which was also eventually chosen, was to move the code for drawing grid boundaries into the rendering class, where it really belonged.

\begin{figure}[ht!]
\centering
\includegraphics[width=1.0\textwidth]{pics/game8.png}
\caption{Draw grid function in action, highlighting cells with sprites}
\label{fig:game8}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=1.0\textwidth]{pics/game10.png}
\caption{Game paused while accelerating and shooting towards an enemy ship}
\label{fig:game10}
\end{figure}

The event handling for key-presses could not be extracted since they relied on being executed in the jQuery context for the events to be registered correctly. The event handles had dependencies on local variables in main.js so in order to extract the code into a separate class these local variables would have had to be made global, which would undermine the design by introducing even more global state. Based on this, the event handling code was left without unit tests, leaving it to integration and system testing to detect possible defects such as improper changes to global variables that were used in the event handler.

The major problems however were not of this technical sort, directly related to \gls{js}, but rather had to do with basic testing principles and how the tests were written. The original purpose of the testing was to provide a safety net for further improvement of the game, but it ended up as over-specification because many tests became implementation specific rather than based on what the code should achieve. Other tests, that were more behaviour and feature oriented, instead had issues with insufficient stubbing and cleanup, causing dependencies between tests so that they would fail or pass for the wrong reasons. The insufficient stubbing and cleanup was largely rectified by executing each set of specs in isolation with just the unit under test, without loading any dependencies before stubbing them, and ensuring that any side effects from running the specs affected only a certain namespace, which could then be reset between each test execution. The problem with the test locking the implementation through over-specification remained however.

A notable testability problem was to automate end-to-end testing. The game involved many nondeterministic aspects in how the asteroids moved that would be awkward to control in the tests, so it was decided not to attempt this kind of testing. If it had been attempted, there would probably have been technical challenges in controlling the game, and areas that would have required careful consideration such as what to base the assertions on in the tests. One tool that looked promising was js-imagediff\footnote{\url{https://github.com/HumbleSoftware/js-imagediff}}, which includes a \texttt{toImageDiffEqual} Jasmine matcher for comparing two images as well as utility methods for producing images of an application that uses canvas. It could probably have been used to avoid having to construct fake objects manually for the canvas in unit tests as well. A similar useful module is Resemble.js\footnote{\url{https://github.com/Huddle/Resemble.js}}, that has advanced image comparison features that are useful together with PhantomCSS\footnote{\url{https://github.com/Huddle/PhantomCSS}}.

\subsubsection{Design Considerations}
\label{ssubsection:designconsiderations}

Because there was no documentation available and the application was sufficiently small, \emph{test plans} were written in the form of source code comments about which tests that could be written, in the \gls{spec} files for each class. Arguably, it could have been beneficial to write actual \glspl{spec} but with empty function bodys, but that might would have given an impression that they were passing rather than waiting to be written.

Each function was analysed with respect to its expected behaviour, such as adding something to a data structure or performing a call with a certain argument, and then a short sentence described that behaviour so that it would not be forgotten when writing the actual tests later. Since tests are typically small, one might think that it could be a good idea to write the tests directly instead of taking the detour of writing a comment first, but a comment is inevitably slightly faster to write than a complete test, since it makes up for fewer lines of code and the implementer avoids the risk of getting stuck with details about how to write the test.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{pics/coverage.jpg}
\caption{Tests should be based on desirable behaviour}
\label{fig:coverage}
\end{figure}

Regardless of which tools are used, it is important to remember that testing strategies matter. For instance, coverage should not be a goal in itself, since that will tend toward tests becoming more implementation specific than they would if instead focusing on what the application should do from a high level perspective. When writing tests for the finite state machine (FSM) in the asteroids.Game object of the asteroids application, it took almost 100 lines of test code to achieve clause coverage \cite[p.~106]{AmmannOffutt} for 18 lines of production code (asteroids.Game.FSM.start), see commit 61713c of \url{https://github.com/emilwall/HTML5-Asteroids}. This is not that much, but it didn't provide much value either (except for increased confidence and safer future modification possibilities) as it did not lead to any changes.

Even if attention is paid to what would be most valuable to test, modifying the application to enable these tests will often have an impact on the design. If the code has to be modified in a way that severely breaks the original design and exposes variables that should not be used from any other part of the application, one of two things apply: either you should not test the code in the way you are trying to, or you should rethink the responsibilities of that code. Perhaps it is doing more than one thing, and it is the secondary thing that you feel that you want to test. No wonder then that it feels wrong to expose certain aspects of it, they might not be the primary purpose of that code segment, just an implementation detail. The key-press event handlers mentioned earlier is one example of this -- the mechanism of event handling should probably have been kept separate from the settings and controls logic of the application.

Global state should not be modified by constructors. For instance, when extracting code from main.js into rendering.js, part of that code was involved with initiating the grid, which is shared between all the sprites in the application (through their prototype), which meant that the grid was not defined unless the rendering class had been instantiated. This imposed a required order in which to run the tests and is an example of poor maintainability and design.

The object orientation of the asteroids application provided useful structure and was a rather good fit since it really did contain natural abstractions in the form of objects. An interesting thought is whether the code would have been more testable if it had been written in a functional style. Presumably, there would have been less outer dependencies to locate and isolate in the tests, or less work to instantiate the objects. \cite[question~26]{Ahnve}

\subsubsection{GUI Testing Considered}
\label{subsec:asteroids}

As mentioned in section~\ref{subsec:browserautomation}, Selenium is currently the most popular tool for browser automation. Sometimes it is the only way to test a \gls{js} of an application without rewriting the code \cite[question~43]{Stenmark} (see section~\ref{subsec:openspace} for further discussion) but then tests tend to be brittle and provide little value. In general, since Selenium tests take such time to run they should only cover the most basic functionality in a smoke test fashion \cite[questions~16-17]{Stenmark}\cite[question~21]{Ahnve}. Testing all possible interaction sequences is rarely feasible and should primarily be considered if it can be done fast, such as in a single page application (SPA) where sequences can be tested without reloading the page. \cite[question~44]{Edelstam}

With Selenium IDE (again, see section~\ref{subsec:browserautomation}), there is a possibility of recording a certain interaction and generate code for it. In general, this has has potential of being used to reproduce bugs by letting the user that encountered the bug record the actions that led to it, and communicate with a developer so that proper assertions are added at the end of the sequence. This has some interesting implications, but experience shows that it is hard to do it in practice since it requires the users to be trained in how to use the plugin and to invest extra time whenever an error occurs \cite[questions~42-43]{Edelstam}. In the case of the asteroids application, it would not work at all, since it does not record mouse movements but merely tracks which elements are interacted with and how.

As a consequence of being unable to extract all code from main.js into testable classes, selenium tests were indeed considered as a last resort of testing its behaviour. This would be a reasonably sound usage of selenium because main.js is basically the most top level part of the application and as such can be more or less directly tested with decent coverage, provided the issues with non-determinism would somehow be dealt with. Selenium just recently included the most basic support for testing canvas elements \cite[p.~165-166]{Selenium2}. However, by the time this was considered there was no time for more experimental work with the application.

Using Mocha with Zombie.js (or Phantom.js) as described in \emph{Using Node.js for UI Testing} \cite{UsingNode} was also considered, but deemed as infeasible because that would require more fine grained control over the interaction with the canvas element than what is possible with these tools.

\subsection{Tool Issues}

Selecting which tools and frameworks to use usually has to be done fast and then you have to live with the decision. In this subsection, we will look at some issues that you might want to be aware of.

\subsubsection{JsTestDriver evaluation}
\label{subsec:jstestdriver}

When setting up JsTestDriver\footnote{\url{https://code.google.com/p/js-test-driver/}} (JSTD) with the Jasmine adapter there are pitfalls in which version you're using. At the time of writing, the latest version of the Jasmine JSTD adapter (1.1) is not compatible with the latest version of Jasmine (1.3.1), so in order to use it you need to find an older version of Jasmine (such as 1.0.1 or 1.1.0) or figure out how to modify the adapter to make it compatible. Moreover, the latest version of JSTD (1.3.5) does not support relative paths to parent folders when referencing script files in jsTestDriver.conf although a few older versions do (such as 1.3.3d), which is a problem if you want to place the test driver separate from the system under test rather than in a parent folder, or if you want to reference another framework such as Jasmine if it is placed in another directory. % Glossary items for JSTD (and maybe Jasmine, Mocha, etc. too?)

One has to be careful when adding code to beforeEach, setUp and similar constructs in testing frameworks. If it fails the result is unpredictable. At least when using Jasmine with JsTestDriver, not all tests fail despite beforeEach causing a failure, and subsequent test runs may produce false negatives. This is likely due to optimisations in the test driver or that the \gls{sut} contains globally defined objects (rather than constructors). game.js contained such a globally defined object and its tests commonly failed unexpectedly after some other test had failed, and continued to do so in subsequent runs even after the failing test has been fixed. Restarting the test driver and emptying the cache in the captured browser temporarily solved the problem, but was time demanding.

When resuming from sleep on a mac the server needs to be stopped and the browsers need to be manually re-captured to the server, or else the driver hangs when trying to run the tests. This is both annoying and time consuming. However, the problem is not present on a windows machine (and it might not be reproducible on all mac machines either). In the interview with Johannes Edelstam, he agreed that this is one example of something that deters people from testing \cite{Edelstam}.

Definitions are not cleared between test runs, meaning that some old definitions from a previous test run can remain and cause tests to pass although they should not because they are referring to objects that no longer exists or that tests can pass the first time they are run but then crash the second time although no change has been made to the code. Some of these problems indicate that the tests are bad, but it is inconvenient that the tool does not give you any indication when these problems occur, especially when there is false positives. This was why reset.js was needed (see section~\ref{ssubsec:gettingstarted}).

If there is a syntax error in a test, JSTD still reports that the tests pass. For example:

\begin{verbatim}
setting runnermode QUIET
...................................
Total 35 tests (Passed: 35; Fails: 0; Errors: 0) (23,00 ms)
  Chrome 27.0.1453.94 Windows: Run 36 tests (Passed: 35; Fails: 0;
Errors 1) (23,00 ms)
    error loading file: /test/sprites-spec/sprite-spec.js:101: Uncaught
SyntaxError: Unexpected token )
\end{verbatim}

As a developer, you might miss the ``error loading file'' message and that not all 36 tests were run, because the first line seems to say that everything went fine. Sometimes Jasmine does not run any test at all when there is a syntax error, but does not report the syntax error either. It is therefore recommended to pay close attention to the terminal output and check that the correct number of tests were run rather than just that there was no failures. This is impractical when running the tests in a \gls{ci} build because the build screen will typically display success even if no tests were run. It can be of help to keep a close look on the order in which files are loaded and also to keep the console of a browser open in order to be notified of syntax errors \cite{MikeJansen}.

\subsubsection{sinon.JS}

Proper stubbing of dependencies can be a daunting task, and is closely connected to mocking and dependency injection. In \gls{js}, dependency injection is harder than in for instance C or Java because of the absence of class interfaces. The sinon.JS framework does make it easier to stub compared to if done manually (which on the other hand is exceptionally simple to do with \gls{js}) but there are still tradeoffs between dedicating many lines of code to stubbing, quite often having to repeat the same code in multiple places, or risk introducing bugs in the tests themselves.

Deficiencies such as these are important to note because they pose potential reasons to why \gls{js} developers don't test their code. If using the tools and frameworks is perceived as cumbersome and demanding, fewer will use them and those who do will consider it worth doing so in fewer cases.

When a function is defined within a constructor it is hard to stub unless you have an object created by the constructor available. In some cases you don't because the system under test creates an instance by itself and then you are (as far as I know) out of options except for stubbing the entire constructor (this produces a lot of code in the tests) or changing the system under test to increase testability, for instance by having the instance passed as an argument (which allows for dependency injection but can be odd from a semantic point of view) or defining the functions that you need to stub on the prototype of the constructor instead of in the constructor (which allows for easy stubbing but is less reliable since another class/object can modify the function as well). Often it is possible to come up with a way that increases testability without having a negative impact on readability, performance, etc. of the system under test, but not always so. Regardless, this requires a skilled programmer and effort is spent on achieving testability rather than implementing functionality which may feel unsatisfactory.

JsTestDriver is not perfect. When refreshing and clearing the cache of a captured browser, you have to wait for a couple of seconds before running your tests or else the browser will hang and you have to restart the server. This wouldn't be such a problem if it wasn't because definitions from previous test runs remain in the browser between runs. For instance, if a method is stubbed in two different tests but only restored in the one that is run first, the tests will pass the first time they are run but then fail the second time. Realising that this is what has happened is far from trivial so as a beginner you easily get frustrated with these small issues, since you might refresh the browser quite frequently in the process of finding out.

Having spent many hours debugging, I finally decided to do a thorough check that no test depended on another test or part of the application that is not supposed to be tested by a specific test. In short, I wanted to ensure that the tests I'd written were truly unit tests. In order to do this, I created a copy of the application repository, deleted every file in the copy except for one test and the corresponding part of the application. Then I configured a JSTD server with a browser to run only that test, and repeated the process for every test. This method does not guarantee absence of side effects or detecting tests that do not clean up after themselves, but being able to run a test multiple times without failing, in complete isolation with the part of the application that it is supposed to test, at least gives some degree of reassurance that all external dependencies have been stubbed. If any external dependency has been left unstubbed the only way for the test to pass is if the code exercising that dependency is not executed by the test, and if a test does not clean up after itself it is likely to fail the second time it runs although this too depends on how the tests are written.

\subsection{Lessons learned}

In this subsection, problems that are common to not just the asteroids application and the selected set of tools and frameworks, are highlighted. There are typically ways to work around at least some of these problems.

\subsubsection{General issues with adding tests to an existing application}

When adding tests to an existing application, it is easy to lose track of what has and what has not been tested. Having access to a functional specification of the application can be of help but it might be unavailable, incomplete or outdated. Then you have to make a specification of your own, in order to be systematic about what tests you write. This can be done top-down by looking at user stories (if there are any), consulting a user experience expert, talking with the product owner and the users (if any) or identify features to test through manual testing. It can also be done bottom-up by looking at the source code that is to be tested and come up with ideas regarding what it appears like all the functions should be doing. The latter is what was done before adding tests to the asteroids application, as was mentioned in section~\ref{ssubsection:designconsiderations}

\gls{tdd} can be applied even on an application that previously has no tests, by writing tests that document any changes you make to the application, and isolates every bug you might find. One should be careful not to break existing functionality, but if that happens then it can be useful to prevent the bug from reappearing by writing a test for it. This is best done before fixing the bug so you can watch the test fail, increasing the chance that the test is correct and ensuring that the same bug is not introduced again without anyone noticing. Writing tests for an application in its existing form before changing it will increase understanding of how it works and reduce risk of breaking existing functionality. However, the awful truth is that sometimes code needs to be refactored in order to make it testable, as was described in section~\ref{ssubsec:attemptsandobservations}.

Traditionally, coverage criteria has been a central concept in software testing and is still today in many organisations. When doing \gls{tdd} however, the need for thinking in terms of coverage is reduced as every small addition of functionality is tested beforehand. The goal is not to test specific implementation details because that will only make the system harder to change. If a certain function feels complex and likely to contain bugs, the recommended way in \gls{tdd} is not to test it more carefully to compensate, but to refactor in small steps and test new components separately, rather than trying to achieve different kinds of graph and logic coverage for the complex function. When adding tests in retrospect it makes more sense to think about coverage, which may be done when the system is starting to feel complete in order to reduce risk of bugs. There are various tools available for ensuring that relevant parts of an application are exercised by tests and it is often relevant to design tests based on edge cases and abnormal use. As a tester, it tends to pays off having the attitude of trying to break stuff instead of just testing the so called happy flow. Different types of coverage criteria can help in formalising this, as described in Introduction to Software Testing by Ammann and Offutt \cite{AmmannOffutt}.

% TODO Add citation for that coverage criteria has been and still is a central concept in software testing
\label{subsec:coveragecriteria}

\subsubsection{Stubbing vs refactoring}

If refactoring is required in order to make code testable, the architecture likely needs to change. Sometimes introducing a new object or changing how some dependency is handled is all that is needed. The important thing is to not continue in a direction that will cause the codebase to deteriorate over time \cite[question~34]{Stenmark}. If a lot of work goes into mocking dependencies (see section~\ref{subsec:mocking}) then the tests take longer to write, require more maintenance and the need for integration tests increases. The proper remedy is usually not to reduce the isolation of the unit tests but to refactor the architecture so that each unit becomes easier to isolate. \cite[question~42]{Stenmark}

Sometimes it can be hard to know whether or not to stub library functions, such as those supplied by jQuery that have no side-effects, or if you should trust that they behave as expected and steer the control flow via their input and the global state instead. The same applies to simple functions you have written yourself that you think are free of bugs. Not stubbing external dependencies leads to fewer lines of test setup and teardown code and usually better test coverage but can also impose a danger of the unit tests becoming more brittle and similar to integration tests. Apart from the technical considerations of how decisions regarding stubbing affects how the test works, there are readability benefits in reducing the number of test setup code lines on one hand and making the test context explicit on the other.

When an application is tightly coupled, stubbing easily becomes a daunting task. What you end up with is deciding whether you should compromise the unit tests by keeping the level of stubbing to a minimum, refactor the code to reduce the amount of calls that needs to be stubbed, or engage in voluminous stubbing. The first alternative bodes for unstable tests that might fail or cause other tests to fail for the wrong reasons. Refactoring might introduce new bugs and should only be considered if it significantly improves the design. Stubbing all dependencies might result in an impractical amount of setup code with complicated testing. The only way to avoid this dilemma is \gls{tdd}, which would enable more aggressive refactoring. This tradeoff had to be made when tests for classes that depended on the Sprite class were written. The Ship class used Sprite both for its ``exhaust'' attribute and for its prototype. A common principle is that constructors should not modify or depend upon global state. Luckily, the Sprite constructor adhered to that principle, so in this case not stubbing the Sprite class before parsing the Ship class was acceptable. However, all calls to methods defined in Sprite were stubbed, since they were tested separately.

Sometimes during the process of adding tests to the asteroids application, the tests accidentally ended up with incomplete stubbing. Detecting this sort of test smell was non-trivial, because there was no automated way of ensuring that a test accessed only a certain part of the application. It was not an option to manually organise the files to ensure that just the appropriated code was loaded, since attempting to stub a function which is not defined produces an exception in sinon.JS, in order to prevent you from making typing errors. This could be circumvented by manual stubbing, but abandoning sinon.JS for that reason felt inconvenient, especially considering that the separation from the code that should not be executed by the tests had to be done manually, so instead I opted towards simply being more careful not to miss any calls that should be stubbed. A semi-automated approach might be possible, using coverage reports to inspect that only the desired code is executed when a certain test is run, but that was not investigated.

As a programmer you have to be very methodical, principled and meticulous not to miss some detail and write an accidental integration test. Such mistakes leave you with misleading failure result messages and sometimes the tests fail because of the order in which they are executed or similar, rather than because of an actual bug in the \gls{sut}.

\subsubsection{Deciding when and what to test}

Testing is especially important for large applications. It is extra valuable when new functionality is added because it helps to verify that old functionality is not broken and that the new code is structured appropriately \cite[questions~6-7]{Stenmark}. You should then focus on testing behaviour rather than appearance and implementation details \cite[question~10]{Edelstam}. Rather than testing that a certain class inherits from another, test that the methods of that class behaves as one would expect. Whether or not that is dependent on the inheritance patterns is mainly relevant for stubbing considerations -- you may want to replace the prototype of an object in tests so that you can check that there are no unexpected dependencies.

Something that came up during the interview with Edelstam was that when a certain section of code feels hard to test, it is likely that any test you write will become rather brittle as the code changes in the future. The proposed solution was to avoid testing it unless the code can be refactored so that testing becomes easier \cite[question~30]{Edelstam}. When writing the tests for the asteroids application, it was deliberately decided to write tests even when it felt hard, or when it felt like it provided little value, to see whether this made the application harder to test or modify later.

Because unit tests are typically fast, it is common practice to prioritise decent coverage and corner cases in the unit tests rather than in integration, UI, e2e (end to end) and other types of tests. When tests can be executed fast, they are more useful when changes are made to the code. This is especially important when someone other than the person who wrote the code is making the changes, or when some time has passed since the code was written. \cite[questions~22-24]{Stenmark} % Glossary for e2e?

Naturally, the more important a certain functionality is in relation to business value, the more effort should be put into e2e and similar tests related to it. Inherently complex parts of the code and code that is likely to change should be tested by unit tests to allow for refactoring and increase chance of discovering bugs, whereas simple code that is not expected to change can be tested manually. However, it is typically hard to know beforehand if the code you are writing is subject to change, so a compromise by writing a few simple tests for that code may pay off. \cite[questions~28-29 and 33]{Stenmark}

In the end, when deciding what tests to write, one of the most important things are to make sure that the tests are meaningful. They should in some way be connected to the problem that the system solves and describe something that matters rather than just testing methods for the sake of it. \cite[questions~17-18]{Ahnve}

\subsubsection{A Related Experience}
\label{subsec:openspace}

During a talk at a meet-up on python APIs (2013-05-22 at Tictail's office, an e-commerce startup based in Stockholm), the speaker mentioned that their application depended heavily on \gls{js}. It turned out that they had done some testing efforts but without any lasting results. During the open space after the talks, testing became a discussion subject in a group consisting of one of the developers of the application, among others. The developer explained that they had been unable to unit test their \gls{js} because the logic was too tightly coupled with the \gls{gui}. This meant that the only observable output that they could possibly test was the appearance of the web page, via Selenium tests. He sought reassurance that they had done the right thing when deciding not to base their testing on Selenium, which he considered unstable (tests failing for the wrong reasons) and slow. He also sought advice regarding how they should have proceeded.

The participants in the discussion were in agreement that testing appearance is the wrong way to go and that tests need to be fast and reliable. The experience with testing frameworks seemed to vary, some had used Jasmine and appreciated its \gls{bdd} approach and at least one had used Karma (under its former name Testacular). The idea that general \gls{js} frameworks such as AngularJS could help in making code testable and incorporating tests as a natural part of the development was shared and appreciated. The consensus seemed to be that in general, testing \gls{js} is good if done right, but also difficult.


\section{(JS,ML0)Problems in JavaScript testing}
\label{sec:jsproblems}

Considering all the different options in available frameworks, one may be decieved into believing that the main reason why people don't test their \gls{js} is because they are lazy or uninformed. This is of course not the case, there are indeed respectable obstacles for doing \gls{tdd} both in the process of fitting frameworks into your application and in writing the \gls{js} code in a testable way. % Stryk eller konkretisera (men då måste man gå in på ramverk, så att det blir för stort)

In order to get started with testing \gls{js}, you need to understand why you should do it \cite[question~38]{Edelstam} and then practise with katas and begin to use testing in your projects. Coding katas focused on front-end \gls{js} development are not very common, but not impossible to do \cite[question~47]{Ahnve}.

\subsection{(JS,ML0)Asynchronous events}
\label{subsec:asynchronous}

\gls{js} is often loaded asynchronously in browsers to improve performance in page loads and minimise interference with the display of the page. Another asynchronous aspect of \gls{js} are events such as AJAX calls and timer dependent functions. Testing asynchronous events can be hard because execution order is not predetermined, so the number of possible states can be very large, thus demanding many convoluted tests to cover all the relevant cases. It can also be hard to know if the asynchronous code has finished execution or not, leading to assertions being executed at the wrong time.

Two basic approaches to testing asynchronous code are to force the execution into a certain path by faking the asynchrony, and to set up waiting conditions for when assertions should be executed. When forcing the execution path, the code is not executed asynchronously but you can test different scenarios and thereby get a good feel of how the code will behave when run asynchronously. When using waiting conditions, timeouts need to be set for when tests should fail if waiting conditions are not met.

Both Mocha and Jasmine supports waiting conditions, with some differences in syntax and how it works.

A common concern is that the large number of possible combinations in which asynchronous code can be executed leads to a combinatorial explosion. This is true, but it is also true that if there are few dependencies between the asynchronous code and if they do not share resources, the risk for data races and similar issues is smaller. Therefore, an important part in achieving testability of asynchronous code is to strive for such designs.

The issue of testing asynchronous code is not unique to \gls{js}, e.g. Erlang message passing is also asynchronous, and has been used in systems with high demands on stability. One may ask how Erlang programmers test their code, answering that question and relating the answers to testing of \gls{js} could be an interesting study of its own. Many other languages with support for multiple threads or processes arguably have this issue as well.

JavaScript Unit Testing \cite{JSUT} goes into some detail of asynchronous testing.

Blog posts (these should be in the bibliography and perhaps mentioned under previous work):
\begin{verbatim}
http://lostechies.com/derickbailey/2012/08/18/jasmine-async-making-asynchronous-testing-with-jasmine-suck-less/

http://lostechies.com/derickbailey/2012/08/17/asynchronous-unit-tests-with-mocha-promises-and-winjs/

http://www.htmlgoodies.com/beyond/javascript/test-asynchronous-methods-using-the-jasmine-runs-and-waitfor-methods.html#fbid=eILq_Eu-N_I

http://www.htmlgoodies.com/beyond/javascript/how-mocha-makes-testing-asynchronous-javascript-processes-fun.html

http://blog.caplin.com/2012/01/17/testing-asynchronous-javascript-with-jasmine/
\end{verbatim}

\subsection{(JS,ML0)DOM manipulation}

A key to successful testing of \gls{js} with \gls{dom} manipulation is to separate the logic from the \gls{dom} manipulation as much as possible. One technique that was presented at the XP conference in Trondheim 2010\footnote{Agile Processes in Software Engineering and Extreme Programming 11th International Conference, XP 2010, Trondheim, Norway, June 1-4, 2010, Proceedings} was to define a view layer of jQuery-related and similar code, and update the view as a separate task in the business logic. This allows for test driven development to be carried out much more effortlessly than if the logic is tightly coupled with the \gls{dom} manipulation. \cite[question~4]{Ahnve}

You can test your \gls{dom} manipulating code if you really need to, but be aware that there are many implementation differences depending on which browser or testing environment the tests are run in (on the other hand, this is one of the reasons why you might WANT to test the \gls{dom} manipulations) and that it can be hard to keep test fixtures up to date.

There are free cloud-based tools such as TestSwarm that are designed to help you with running the tests in many different browsers, or you can do it manually using JsTestDriver, which also has built in support for setting up
\gls{testfixture} DOMs. One way is to use different Selenium drivers, then you can check almost anything, but beware that the tests become even slower so even for a medium sized application you might end up being unable to run the tests more than a couple of times per day.

\subsection{(JS,ML0)Form validation}

Validation of forms is commonly done both server and client side so that users do not have to wait for requests to be sent to and returned from a server in order to validate input, while maintaining the security that server side validation offers. Preferably, these validations are automatically kept in sync by frameworks and ideally it should be enough to test the server side validation, but that is sometimes just as hard and to be sure you should test both.

Tools for client side form validation include Parsley\footnote{\url{http://parsleyjs.org}, \url{https://github.com/guillaumepotier/Parsley.js}}, validate.js \footnote{\url{http://rickharrison.github.io/validate.js/}, \url{https://github.com/rickharrison/validate.js}} and the jQuery Validation Plugin \footnote{\url{http://jqueryvalidation.org}, \url{https://github.com/jzaefferer/jquery-validation}}. It is also possible to use the built in support in HTML5 (which however will not work in old browsers) or implement custom validation that is called with onsubmit or onblur. However, form validations should not just have to be implemented, they should be tested too.

You typically need to test for both type I and type II errors (false positives and false negatives). If forms are prevented from being submitted because of an error in the validation (type I error), the number of successful conversions (website goal achievements) is likely to decrease. Allowing invalid forms to be submitted (type II error) can either cause prolonged feedback for the user, loss of form data so the user has to re-type and loss of context due to loading an error page resulting in decreased conversion rates or, if the validation fails at the server side too, security issues.

A real life example of a false positive in form validation is that a personal number is interpreted wrongly, producing an error message falsely claiming that that the applicant is not old enough. In such cases, most people might choose another provider of the service. Proper testing of the form validation could detect this at an earlier stage, thereby avoiding loss of potential customers. \cite{TwitterMe}

A similar bug was observed this year on a scandinavian internet bank. When entering the personal ID it got copied to the password field, and when entering the password it got copied to the personal ID field, so you had to use a cellphone identification service in order to log in. Such a thing would not happen if the bank had automated cross browser testing of the forms. \cite[question~38]{Ahnve}

So how can form validation be tested? Selenium is one option, to enter form data and submit and then check that the proper validation error was printed out at the right location. A benefit with this approach is that a user interaction can be closely imitated. A problem is that if you have many validations that you need to test, these tests tend to take a lot of time.

Depending on which framework you're using, you might have different options for testing. Just testing the validation rules without involving the forms at all can be of great value, e.g. if you are specifying the validation using regular expressions, hopefully you can expose these for isolated testing so that you can avoid testing all the validation in slow integration tests. This will not detect runtime errors or similar for the actual validation, but for complex validation such as testing which strings match a certain regular expression, you are typically better off with fast unit tests, which can be complemented by a smaller number of smoke tests for the integration.

A common case is that you are using another programming language than \gls{js} for the back-end, and you specify the validation rules in that language and let for instance a jQuery plugin generate validations for the front-end. Here you can hopefully put regular expressions and the like as publicly available variables and access them directly in tests, whereas the plugin will have to be tested separately, using perhaps a single test that just checks for existence of validation.

\subsection{(JS,ML0)GUI testing}

A \acrfull{gui} is usually the top layer of an application and is therefore a common target for system testing. It can consist of many components and typically provides a large number of possible interactions, which is why \gls{gui} testing is important, but also part of why it can be a time consuming activity both in terms of preparing and executing tests.

\subsubsection{(JS,ML0)Application differences}

A rather common opinion is that you should avoid automatic testing of an application's visual components. However, the value provided by \gls{gui} tests depends on how important the \gls{gui} is for the application. In web applications, the \gls{gui} and sequences in it are commonly more complex than the logic underneath, so testing the \gls{gui} makes more sense for such applications than for typical desktop or financial applications where backend logic tend to be more important. \cite[question~21]{Ahnve}

Before deciding on whether you want to do automated \gls{gui} testing or not, think carefully about how important the \gls{gui} and its cross browser compatibility is to you. If you feel that the exact appearance is irrelevant, then there is little sense in doing it. If you on the other hand feel that it would be valuable but have concerns about the technical aspects of it, you should not dismiss it too easily.

\subsubsection{(JS,ML0)Regression testing of GUI}

\gls{gui} testing should not be confused with integration testing. Testing graphical elements in tests that depend on system integration are hard to maintain and can fail for too many reasons. Instead of executing a sequence before making assertions about graphical components, you may benefit from keeping assertions simple in an integration test. The \gls{gui} testing can then be performed in a unit testing fashion as a separate activity, by creating a \gls{testfixture} that mimics the state of the application the way it looks after a certain sequence has finished.

This approach has a fairly obvious risk. What if a \gls{testfixture} fails to simulate an application state properly? You could detect this by comparing the application state with the corresponding \gls{testfixture} in an integration test, deliberately introducing dependencies between tests. It might also be hard to construct such a \gls{testfixture} to begin with, or you may feel that it introduces too much maintainability burden to have a \gls{testfixture} that needs to be updated every time a relevant change occurs, in which case you have little choice but to either write bad \gls{gui} tests or skip writing them altogether.

Just as you might decide for or against implementing rounded corners that work in old browsers, based on the number of clients actually using such browsers and the maintenance cost of implementing it for every picture of a site, there is always a tradeoff regarding whether or not to test such graphical elements with automated regression tests or not. In the end, all that matters is cost versus gain, no matter what personal beliefs and preferences might be involved from the product owner or developers' personal point of view. \cite[questions~40-44]{Ahnve}

\subsubsection{(JS,ML0)Automation of GUI testing}

Manually testing a web page with all the targeted combination of browsers, versions and system platforms is usually not a viable option \cite{TestSwarm} so multi-platform automated testing is required. This is true in general for web application testing, but also holds for \gls{gui} testing in particular, regardless of whether the \gls{sut} is a web application or not. Since the appearance of a \gls{gui} can depend on factors such as configuration of hardware, operating system, version of third party software, etc., and because of the many possible sequences of interactions to test, it is rarely feasible to test all combinations and unless the testing is automated on multiple platforms, you will be unable to test even a small fraction of them.

\gls{gui} testing can be done in a semi-automated fashion. There are ways of producing snapshots of how websites look in different browsers and you should be able to have a program compare such snapshots to spot differences with previous versions, that can be reviewed by a human. This would reduce the need for manual testing of the \gls{gui}, since you get a good indication of which parts have changed. % http://piwik.org/blog/2013/10/our-latest-improvement-to-qa-screenshot-testing/

Apart from writing tests in code using for instance \gls{tdd}, there are techniques for automatically generating tests using crawlers or artificial intelligence. These techniques will by nature have more focus on coverage and detecting runtime errors than to simulate the behaviour and expectations of users. There has been quite a lot of research in this area \cite{AutomatedTesting}\cite{AutomatedAcceptance}\cite{AutomatedAjax}.

Another technique is capture-replay, which is based on the concept that a user interacts with the \gls{sut} and records the actions. Selenium IDE provides this possibility, and there are other tools as well that can do this both for web and desktop applications.

A more modern approach is model based (FSM) \gls{gui} testing, where graphs are used to represent how the \gls{sut} can be interacted with and which states should be possible to reach. From such a graph, test cases can be generated.

\subsection{(JS,ML0)Private APIs}

Most problems with APIs, both external and private, are similar to problems with any external dependencies that you do not have full control over. The tests need to be kept up to date and in order to do so, integration tests are needed. Sometimes, testing towards a true implementation is problematic because it takes too long time, manipulates data that should not be touched or because the owner of the API does not accept large amounts of traffic or requires payment for it. Then you typically run the integration tests less often and stick to mocked APIs with fake objects to compensate. \cite[questions~19-20]{Stenmark}

Testing private APIs differs from testing calls to an external API in that the latter are often well documented, versioned and rarely changes. When using an external API, the main concerns are making sure that the correct version is used and that the API is called in a correct way with respect to that version. A private API on the other hand may change frequently which means that the fake objects representing requests and responses of the API in other tests need to change as well. Private APIs need to be tested because endpoints frequently change and may be used in several places. It is then important to have integration tests that can detect when the fakes need to change. \cite[questions~34,~36]{Edelstam}

Introducing versioning for private APIs is not always feasible, for at least two reasons. There can be difficulties in keeping track of the different versions and there can be a risk of hampering development since the parts that use the API are dependent on the new version of the API to be released before they can be updated. However, there exists testing techniques that can be employed regardless if versioning is in place or not. One such technique is to write tests for what characterises an object that the API serves as an interface for, to be able to determine if an object is a valid response or request. These tests can then be used on both real objects and on the fake objects that are used in other tests, which means that when the API changes the tests can be updated accordingly and then the tests involving the fake will fail. Changing the fakes so that the tests passes will hopefully result in that any incompatibility is discovered since the fakes are used in other parts of the testing suite as well. \cite[question~34]{Edelstam}

Testing a private API is not the same thing as testing a private method. In \gls{js} there is formally no private methods, only functions not exposed to the current scope, nevertheless methods that are not intended to be called from the outside can be considered private. It is a common principle that you do not test a private method, if you feel the need to, then that method is probably too complex to be private and the functionality that it is meant to provide should be moved into a new object or similar. Private methods represent implementation details, tests should be focused on behaviour and functionality, not implementation. \cite[questions~62-63]{Edelstam}

\section{(JS0,ML)Problems in testing of Standard ML}
\label{sec:smlproblems}

\Gls{sml} and \gls{js} have at least two things in common: they are both functional languages (although JS is perhaps more rarely used as such) and they both have poor testing cultures. This section relates how \gls{sml} testing is carried out today to existing frameworks and formal verification, and introduces a new testing framework that was developed for \gls{sml} as a part of this thesis, inspired by insights gained from looking at popular \gls{js} testing frameworks. In section~\ref{sec:testability}, we discussed how adding tests to an application written without testing in mind can introduce testability issues. Later in this section (~\ref{subsec:tddmooc}) the challenges of writing tests first is described based on a \gls{tdd} approach to solving \gls{sml} programming exercises in a \gls{mooc}.

\subsection{Current Situation}

The SML/NJ and Moscow ML language implementations\footnote{\url{http://www.smlnj.org/svn.html}}\footnote{\url{https://github.com/kfl/mosml}}, and other \gls{sml} projects, have tests for large portions of the code, but there is no standard way of writing the tests. Each module typically ``reinvents the wheel'' by having its own scripts for testing, that is invoked differently and produces different output. If you search the web for ``Standard ML testing framework'' (with quotes, Nov 2013), you get zero results. Dig deeper, and you'll find QCheck/SML (henceforth referred to as QCheck) and a few small testing framework projects without any large user bases (see Appendix). It seems \gls{sml} testing suffers from lack of motivation or knowledge among developers, or availability of good tools. % footnotes after "other projects"

As a programmer, you should strive to do the simplest thing possible within the confines of given constraints, and to reuse as much as possible provided that any reuse candidates meets demands on quality and adaptability. \Gls{yagni} advocates this and is incorporated in many programmers' ways of thinking. When starting to write tests in a project, the pragmatic approach is to adhere to existing codebase conventions, or, in the absence of such, search the web for a suitable testing framework and use it in the project. Why is it that so many end up writing tests without using an existing testing framework? It seems that \gls{sml} developers prefer writing their own testing framework rather than using an existing one.

As indicated by its 30 ``stars'' on github (Nov 2013), the currently most popular testing framework for \gls{sml} is QCheck. However, it is somewhat cumbersome to use in scenarios where property testing is not the main objective. Some may decide against using it because they want something that is simpler, more suitable for \gls{tdd}, or has a different (e.g. \gls{bdd}) style of defining tests and formulating failure and success reports. Without any production quality framework out there that satisfy these basic needs, it may feel like an easier task to do without a testing framework or write a new one than to familiarise with and adapt to an existing one that might turn out as disappointing anyway. The chance of developers deciding to use a framework relies on well prioritised features, active maintenance and a professional, practical or in other ways appealing presentation and reputation.

There are different driving forces behind testing different languages (such as \gls{js} and \gls{sml}), because of the background and motives of people using them, and because of the diversity of the applications in which they are used. This partially explains why QCheck as the only truly popular testing framework for \gls{sml} is based on generating test cases for properties rather than simple expectations/assertion based testing. The developer behind QCheck is Christopher League, a professor at LIU (Long Island University) Brooklyn, so his academic background in computer science is one of the answers. It could be that mathematicians are drawn to the language and are inclined to think in terms of properties and how they can be falsified rather than stating expectations on program state or output.

Another possible reason is that testing for expected output is easy even without the use of a testing framework, whereas QCheck solves problems that are not as easily solved ad hoc. Since \gls{sml} has scripting capabilities, it is relatively easy to write and run tests without a testing framework, compared to doing it in languages that are more verbose and less often used in an interpreted setting, such as Java. Adding a testing framework becomes a bigger deal in \gls{sml} for this reason. Possibly people simply do not find it worth their time to learn testing frameworks just to get readable tests and good failure reports, but instead are content with simply declaring tests as variables that evaluate to true or false, or similar. The consequences of doing this, however, are likely to include more debugging and risk of not noticing when tests fail, or ending up spending a lot of time scrolling through results of the ad hoc tests.

Yet another issue is the methods for distribution. In Node.js installing a package is very easy with the NPM and there is a central place to find popular node modules (\url{https://npmjs.org}). For \gls{sml} there is the SML/NJ Compilation Manager (CM) and Smackage to choose between, and no central repository for modules except for github. Authors of testing frameworks might find it hard to decide which package manager (CM or Smackage) to use, figure out how to use it, or reaching out to users -- which may also find learning how to use a package manager too cumbersome. Reputation, availability, ease of installation and use, and nice presentation may be essential for getting people to use a framework. % Node.js och NPM => glossary eller technical background?

Certain testing frameworks provide a watch capability, that runs tests whenever a change to the \gls{sut} is saved to the file system. In order for this to scale, only relevant tests should be run, or else the benefit of instant feedback is lost. Relevant tests are typically the ones that could be affected by changes in the specific files that have changed, but may also be selected based on speed or past failure rate. The idea is similar to having a build screen for a \gls{ci} build, making not only the tests automated but also the task of running them frequently, and helps ensuring short cycles of writing a failing test, making it pass and refactor.

Mocking and stubbing may not be as important in a functional language like \gls{sml} compared to more imperative languages, since dependencies can often be passed as arguments and functions tend to behave the same way between calls given a certain input. Doing call assertions is as much about architecture as it is about state, and state is mainly kept in which data flows in and out of functions in a functional program, rather than in values of mutable variables as is often the case in imperative programs. However, there are certainly places where it would be appropriate to stub or mock a dependency in a function without having to redesign the function so that the dependency is passed as an argument, and in those cases the lexical scope rules and immutability of \gls{sml} makes it hard to do so. It might be possible to implement a mocking framework that parses the code of a \gls{sut} and produce new bindings that have the desired dependencies and assertion capabilities, but such a framework would have to be quite sophisticated in order to not break the code and is therefore outside of the scope for this thesis. A simpler approach would be to ensure that any dependencies are bound to stub functions before the \gls{sut} is loaded, so that the stub definitions are used instead. % Write about this in the Future section.

The strict type system of \gls{sml} does not provide any generic toString method for the primitive types int, real, bool, etc. This means that in order to get good error reports from equality \glspl{matcher}, you either have to pass a type specific toString function, define \glspl{matcher} for each type: toEqualStr, toEqualInt, etc. or let the REPL (Read-Eval-Print-Loop) display the results (which is done differently depending on the \gls{sml} implementation). You could also define the \glspl{matcher} in separate namespaces (structures), as is done in smell-spec\footnote{\url{https://github.com/davidpdrsn/smell-spec/assertions.sml}}.

QuickCheck, the Haskell predecessor of QCheck, resolves this by using the \texttt{where} keyword to specify the types of properties\footnote{\url{http://www.cse.chalmers.se/~rjmh/QuickCheck/manual\_body.html\#4}}, which is elegant but unfortunately would not solve the problem in \gls{sml} because types and structures can not be passed as arguments to functions. % TODO gör om till riktig referens, helst annan källa

\subsection{(JS0, ML)Formal Verification}
\label{subsec:formalverification}

It has already been mentioned that QCheck is based on generating tests for properties defined in terms of the code. A similar but more rigorous approach is formal verification, which is performed by manually constructing mathematical proof of correctness, theorem proving, or deciding whether a model satisfies a given property (known as model or property checking). Model checking is most easily automated, either by specialised software such as CADP\footnote{\url{http://cadp.inria.fr}}, UPPAAL\footnote{\url{http://www.uppaal.org}} and many more\footnote{\url{http://en.wikipedia.org/wiki/List_of_model_checking_tools}} (a couple of them written in \gls{sml}) or by using a constraint solver (see \url{http://www.cs.utah.edu/formal\_verification/pdf/ligd\_thesis.pdf}. % Add Formal verification to glossary. Make a real reference of thesis.

A problem with formal verification is its cost, because it can be so time consuming and requires special knowledge. This implies that unless bugs are potentially very severe, formal verification may not be worthwhile. On the other hand, if bugs have potential disastrous consequences, formal verification is the standard way of guaranteeing that certain properties hold, or prove the absence of certain defects. Testing on the other hand is mainly useful to detect bugs that are likely to occur. Tables \ref{tab:fvt} and \ref{tab:fvt2} (pages \pageref{tab:fvt} and \pageref{tab:fvt2}) illustrate these differences between formal verification and testing. The tables are taken from lecture slides by Dr. Radek Pelánek\footnote{\url{http://www.fi.muni.cz/~xpelanek/IA158/slides/verification.pdf}}, used with permission.

\begin{table}
  \centering
    \begin{tabular}{ l | l l}
    & Formal verification & Testing \\ \hline
    Finding bugs & medium & good \\
    Proving correctness & good & useless \\
    Cost & high & low \\
    \end{tabular}
  \caption{Formal verification is expensive and less useful for finding bugs than testing, but can prove correctness}
  \label{tab:fvt}
\end{table}

\begin{table}
  \centering
    \begin{tabular}{ l | l l}
    & Likely & Rare \\ \hline
    Harmless & Testing & Unimportant \\
    Catastrophic & Testing and FV & \textcolor{red}{FV} \\
    \end{tabular}
  \caption{Formal verification (FV) has a sweet spot when bugs are critical and rare, whereas testing works best when bugs are common}
  \label{tab:fvt2}
\end{table}

An interesting discussion is whether formal verification can have similar effects on modularity and interface design as \gls{tdd}. Isolated units, for example short methods with few dependencies and side effects, are easier to construct proofs for, so modularity can certainly be positively affected by formal verification. It also favours simplicity, for good or ill. Whether it has benefits for interface design depends on what we mean by interface. Formal verification is unsuitable for \glspl{gui} but can help to keep APIs terse.


\subsection{(JS0,ML)DescribeSML}
\label{subsec:describesml}

By the time I found out about existing SML testing frameworks (apart from QCheck) I had already started to write my own. I wanted to use my insights from using JS testing frameworks such as Jasmine and Mocha to write a \gls{bdd} framework which I would use in the Coursera \gls{mooc} course Programming Languages\footnote{\url{https://www.coursera.org/course/proglang}}, as mentioned in section~\ref{subsec:programming}. The working title of it became DescribeSML.

\subsubsection{(JS0,ML)The framework}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\textwidth]{pics/DescribeSML.png}
\caption{DescribeSML file structure and module architecture. \Glspl{spec} are written using Expect \glspl{matcher}, described and organised with Describe and should clauses, and the result of running them is reported by the SpecReporter. The arrows show how data flows through the application when the \glspl{spec} are executed.}
\label{fig:describesml}
\end{figure}

Writing a testing framework in \gls{sml} turned out to be an easier task than anticipated, it took only a few hours of development to get functionality such as \glspl{matcher} for checking equality, membership, regex matching and that certain exceptions are thrown. Nested describe-clauses were also implemented, to allow for neat organisation of \glspl{spec}, and the syntax and failure/success reports quickly became reasonably appealing.

Because of its modularity, DescribeSML could be used in many different ways. The \texttt{Expect} structure in expect.sml could be used on its own as an assertion framework, possibly wrapped or modified to signal success and failure differently than the \texttt{Describe} and \texttt{SpecReporter} structures do. However, the preliminary intention of usage is to define a test suite with the \texttt{suite} function, containing nested \texttt{describe} clauses, each containing \glspl{spec} defined with the \texttt{Describe.should}. The listing in figure~\ref{lst:describesmlexample} (page~\pageref{lst:describesmlexample}) shows an example of how it could look.

\begin{figure}[ht!]
\lstinputlisting[language=ML]{describe-sml-example.sml}
\caption{describe–sml–example.sml}
\label{lst:describesmlexample}
\end{figure}

The source code of the SML testing framework is supplied in the appendix. It can also be found at \url{https://github.com/emilwall/DescribeSML}.

\subsubsection{(JS0,ML)Alternatives considered and discarded}

Although the strict type system, functional style and static analysis of \gls{sml} slightly reduces the need for testing compared to dynamic and imperative languages such as \gls{js}, it inevitably also results in more limiting testing frameworks. It is somewhat impractical in \gls{sml} to state several expectations in a single \gls{spec} or stub functions used by the \gls{sut}. However, this can also be a good thing, since it leads to more concise tests.

One way to compensate for this inflexibility could be to allow shared data between \glspl{spec} (see figure~\ref{fig:data}, page~\pageref{fig:data}). Then it would not be such a problem that all \glspl{spec} have single expectations, since any value that you want to perform multiple assertions on can be computed once and then used in several \glspl{spec}. In an imperative language like \gls{js} this could encourage bad practises and lead to unwanted dependencies but since ML is (almost) purely functional, that is not an issue. Data could actually be shared between \glspl{spec} with a simple let-in-end expression already, but a future version of DescribeSML might implement a \texttt{given} construct to clearly demonstrate the possibility, and to better handle situations where an expression is raised by the expression producing the data.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\textwidth]{pics/data.png}
\caption{Sharing data between tests does not introduce dependencies between them in a functional language such as \gls{sml}}
\label{fig:data}
\end{figure}

Passing a toString function or letting the REPL display the results of an equality \gls{matcher} are probably the only ways to print informative failure reports for user defined datatypes. The downsides are that if a toString function must be passed to each \gls{matcher}, it imposes verbosity and makes the tests harder to read, and if the REPL is used the output can not as easily be written to a file or output as XML or other formats. It could be a good idea to use local definitions of \glspl{matcher} defined for a set of specs, for example:

\begin{verbatim}
describe "the function foo"
    let
        val toEqual = toBeInt
        val longInput = "string"
    in
        [should("return string three times as long as argument", fn _ =>
            expect (size(foo longInput)) toEqual (3 * size longInput))]
    end
\end{verbatim}

However, this leads to increased indentation level and verbosity, and restricts the use of any local (possibly shadowing as in the example) \gls{matcher} definition to the specified type, which is not always particularly convenient. The matchers that are otherwise hard coded in the framework due to the static typing are toEqual, toEqualStr, toEqualInt, etc. The worst part however is defining matchers for lists and tuples because you might end up needing things such as toContainStr, toEqualIntList or toEqualIntTuple5 (for instance, defined for int 5-tuples only) rather than toContain or toEqual which is what you really want. The real issue is getting informative and practical failure messages for all the types, user defined datatypes included, and to ensure that failure messages are output in a way so that you do not miss them by mistake.

Another consideration is that order may or may not matter for matchers of non-atomic data structures. This should be clear so that users of the testing framework are not surprised by the matchers' behaviour, and if one has to be picked over the other it should probably be that order does \emph{not} matter, since the order of the elements can usually be checked anyway, in a way that makes it more explicit that it is supposed to matter. Not checking for a specific order unless necessary might increase the complexity of a matcher but will tend to make tests less implementation specific. A similar issue is whether a toThrow matcher should check the exception message or just the type or name of the exception. DescribeSML checks if the exception message begins with the specified message, but does not require the messages to be exactly equal, in order to allow for a lenient specification of the exception to be thrown.

The naming of bindings is important in any language and a fundamental challenge that you face when learning to use a framework is what its functions are called and what they do. It is not always obvious for the people writing a framework what naming will come forth as the most natural and useful, for example there might be a tradeoff between conciseness and readability when choosing between naming a matcher toBeStr or toBeString. There is an option of binding both names to the same function, but that is considered bad practise because the user might expect that there are differences and the benefit of a uniform or idiomatic way of using the framework is lost.

Names generate assumptions regarding the semantics of a function. For example, in most languages there is a difference between two variables referencing the same object and two variables referencing objects that are equal. In \gls{sml}, there is no notion of objects so this distinction is not as important. Moreover, there can be multiple references to the same value (as in \texttt{val a = ref v; val b = ref v}) and it is also possible to bind the same reference to several names (as in \texttt{val a = ref v; val b = a}), resulting in different results when checking if the references are equal although they do indeed point to the same value, depending on how the references were defined. This gave rise to the question whether there should be a difference between toEqual and toBe, and what the semantics should be. It was considered superfluous to define special matchers just for reference values, since they are used so sparsely and can be tested through dereferencing anyway. Based on that rationale, toEqual and toBe were bound to the same function in the first version of DescribeSML, which simply compared the arguments using the overloaded equality operator of \gls{sml}. It can be argued that it would have been wiser to define just one of them, but it turned out that toBe felt more natural for primitive types such as booleans and numbers whereas toEqual felt more appropriate for composite data types such as tuples and lists.

A simple but important consideration of a testing framework is the phrasing of failure messages. In the first version of the DescribeSML reporter, some messages were formulated as ``expected ``abc'' to equal ``def''''. This made it somewhat unclear what was the expected and what was the actual value. Another bad example would be ````abc'' does not equal ``def''''. Better alternatives include ``expected ``abc", but got ``def"" or ``was ``def" but expected it to equal ``abc"". Here, another consideration is whether the failure reports should indicate the kind of \gls{matcher} used in the test. Sometimes this will make the failure report read better and reveal if the test is using the wrong \gls{matcher}, but it also adds complexity to the testing framework that is probably not worth it if the testing framework becomes harder to use.

Failure messages should be displayed in the order that the \glspl{spec} are executed, and the \glspl{spec} should be executed in the order that they are defined, because even though \glspl{spec} should be written so that the order in which they are executed does not matter it can be confusing for the user if the execution order is not clear.

Writing your own testing framework can be entertaining, inspires to write more tests, and is a good way to hone \gls{tdd} skills. However, if existing frameworks are good enough, writing a completely new one may not be justifiable towards a customer. Since open source is increasingly common, a better approach may be to improve an existing framework.

An intriguing aspect of writing a testing framework is that you can use the framework to test itself. This may sound like circular reasoning, but actually worked quite well to detect defects immediately after they had been introduced and through \gls{tdd} ensuring a well thought out design and providing feedback about which parts are important and easy to use, and which parts are missing or deficient in some aspect. An early version of DescribeSML had a toThrow matcher that reported which exception was expected to be thrown when no exception was thrown, but did not check that a thrown exception was the correct one. This is sometimes known as The Silent Catcher anti-pattern, although it is more commonly introduced by the user of a testing framework than by the framework itself. The toThrow matcher was implemented in an early stage, before the framework was used for \gls{tdd} of itself, so it serves as a good representative of bugs that are easily introduced in the absence of tests.

Implementing the nested describes was perhaps the hardest part of writing DescribeSML since it had to be both recursive and simple to use. The expect-function was written as a one-liner: \texttt{fun expect it f = f(it)}, in order to emulate infix behaviour of the matchers, which were written as curried functions. A curried function evaluates its arguments in turn: \texttt{fun toEqual x y = if x = y then "pass" else "fail"}. An alternative could have been to define expect as the identity function \texttt{fun expect x = x} and then define all matchers as infix: \texttt{infix toEqual; fun op toEqual (x, y) = if x = y then "pass" else "fail"} -- an advantage of this would be that expressions such as \texttt{expect 1 + 1 toEqual 2} would be valid syntax, whereas now you have to add parentheses around the addition.

Aside from these considerations, there were not that many issues with writing most of the matchers. A benefit of this was that the simple ones were so simple that they required virtually no testing, and could safely be used to test the more complex ones.

The first versions of DescribeSML did not have nested describes, in favour of the \gls{yagni} principle, but the need quickly became apparent when using the framework in practise. Collecting specs in different describes would have made it harder to run them all at once and inspect the results, since the failure/success reports would get scattered instead of collected at the end of the output. Having all specs in a single describe was not desirable either, because that reduced the ability to specify their contexts. Nested describes and other ways of improving the usability are especially important when the tester has a small screen so only a small section of failure/success reports can be viewed at a time.

When a test throws an exception, the \texttt{should} function handles it and displays it as a failure (unless the expect toThrow matcher is used), so that the other specs can still be executed. This feature could be misused by writing tests that throws an exception instead of returning a failure message, but this would be bad practise since it might throw another exception than anticipated. The matcher toNotThrow is better to use in most situations, to make the test more precise, i.e. ensure that it fails for the right reason.

\subsubsection{(JS0,ML)Using the framework}

When solving the first assignment, the instructions were often to use the "answer to the previous question". In this case, you would probably want to do a call assertion, which should be possible to do similar to how it is done in JavaScript, temporarily overwrite a definition of the function to stub, use side-effects to store information about calls to it, and then restore it (either automatically after the test has ended if the stubbing framework can be integrated with the testing framework, manually by calling a method, or super-manually by performing the stubbing using "vanilla-ML").

As DescribeSML is currently written, there is no standard way of defining several expectations in a single should. When using the framework, one might feel the need of doing so rather than writing an almost identical spec just for the sake of stating an extra expectation. This could easily be implemented by changing the type of the should function so it accepts a list of expectations instead of a single one, but that would make the syntax of the framework more complicated and it would be harder to determine what the cause of a failure is.

An interesting discussion regarding \gls{sml} testing is whether current frameworks could be merged and unified into new, improved ones with collaborators joining efforts of maintaining them. Possibly, small testing frameworks such as DescribeSML could be made into plugins for others such as qCheck, in order to make them easier to use and provide additional functionality. Getting people aware that the plugin exists and finding ways of integrating might be difficult though. Since qCheck writes its output to stdout, changing the way results are reported in non-trivial. Therefore, modifying both frameworks might be necessary and then it could make more sense to merge small testing frameworks such as sml-testing and smell-spec, and keep them simple.



setup/teardown/beforeEach/beforeAll/afterEach/afterAll-metoder heller inte lika viktiga (just pga att stubbning är en vanlig del av setup), men har fortfarande flera användningsområden

\subsection{(ML,JS0)Test-Driven Development of Programming Assignments}
\label{subsec:tddmooc}

Testing is arguably very useful for educational purposes, both for students when solving coding assignments and for teachers when grading.
A teacher may supply students with small testing suites to build upon, or encourage testing in other ways.
The value for teachers lies in that the students can work more independently and that tests can be written specifically to evaluate solutions.
This can provide a fast and objective evaluation of requirements satisfaction, although style aspects and non-functional requirements are typically not possible to evaluate without manual evaluation.
Introducing testing early is an important step to encourage the understanding of testing principles for students and to improve efficiency for everyone in the long run.

A factor that makes testing especially fitting for programming assignments is that it can help students to understand requirements better.
When requirements are clear and unambiguous, testing will guide the student in implementing them in a structured fashion whereas when they are not, testing can help in detecting where additional information is needed or assumptions have to be made.
It is better that a student finds errors through testing than being informed of them retrospectively in evaluation feedback.

A real world example of \gls{sml} \gls{testing} in an educational context is the Coursera \gls{mooc} Programming Languages\footnote{\url{https://www.coursera.org/course/proglang}}.
In that course (and others, not listed here), students are encouraged to test their code and required to hand in their tests together with their solutions.
The handed in tests are not graded, and not likely to be read by the course administration to any large extent, but the requirement still has an effect on the amount of testing done by the students.

The tests that students write facilitates the coding process and gives confidence to refactor.
They are also useful for the peer reviews, because they provide instant feedback on what behaviour other students might have missed.
For testing to be truly valuable for a student, he or she should be careful not to consider an assignment as complete based solely on a passing test, especially if the test is provided by the teacher.
A complex problem might be harder to test, but also more important.

As previously mentioned, there has been a recent increase in the number of testing frameworks available for \gls{sml}. This might be explained by the need for such frameworks for \gls{mooc} students.

\begin{quote}
``I wrote sml-testing for two reasons, first because I was taking the Coursera course on Programming Languages and didn't find anything else in which to test my code. Secondly because I wanted to explore testing concepts with the use of currying and polymorphic types.''
\end{quote}

-- Kjetil Valle, author of sml-testing

Tests were required not only for \gls{sml}, but also for the two other programming languages in the course, namely Racket and Ruby.
Racket has rackunit as its built in testing framework, which aims to support both novice and advanced programmers but lacks \gls{bdd} support.
Although feature-rich, compared to \gls{js} testing frameworks such as Jasmine and Mocha, rackunit is hard to write readable tests in.
Alternative Racket testing frameworks such as test-engine does not hade \gls{bdd} support either, but there has been efforts to implement it: \url{https://github.com/damienklinnert/lisp/tree/master/bdd}

Ruby has a great testing culture so finding suitable testing tools was no problem for that part, however in the introduction of the first assignment it said:

\begin{quote}
``Because the primary challenge in this assignment is reading, understanding, and extending a program in a new-to-you programming language, we [have] not provided any example tests. We also acknowledge that testing this sort of graphical program is difficult.''
\end{quote}

Indeed it turned out to be difficult to test certain things, for instance that key bindings in the Tk \gls{gui} invoked the correct functions, since no way of triggering the events was found.
In general, however, \gls{tdd} had great benefits in all of the assignments, regardless of programming language.


\section{(JS,ML)Testing culture}
\label{sec:testingculture}

Different people have different opinions about testing: how it should be done, when it pays off and how exciting it is. Some see testing as a tool that helps to provide structure and improve the design of code, whereas others see it as a way to detect bugs and prevent others from braking the code. Most people agree that testing is hard, but many struggle to do it anyway. Attitudes towards testing are essential to whether or not it is carried out or not, and to how worthwhile it turns out to be in the end.

\subsection{(ML,JS1)State of events}
\label{subsec:stateofevents}

Many believe that testing of \gls{js} is uncommon because \gls{js} programmers are often web graphic designers, without that much experience of automated testing. Another common conception is that \gls{js} in practise is usually not testable because it has too much to do with the front-end parts of an application, so tests are inevitably slow, unmaintainable and/or unreliable because of the environment they have to run in. However, lately there seem to have been a turn of events.

The overall quality of \gls{js} code might have improved over the last few years due to improved toolsets and testing methodologies entering the \gls{js} community. The new application frameworks has brought not only technical possibilities (as mentioned in section~\ref{subsec:frameworks}) but has also brought people with background from test-driven backend-programming, experienced with for example Ruby on Rails development. In fact, most of the new frameworks have been developed by people with such background. This has influenced the way \gls{js} code is written and given birth to a novel testing culture. \cite[questions~12-15]{Ahnve}

One may argue that \gls{tdd} forces developers to write testable code which tends to be maintainable, and it may be true in some scenarios, but one has to bear in mind that \gls{js} is commonly used with many side-effects that are not so easily tested. More importantly, it is common to place all the \gls{js} code in a single file and hide the implementation using some variant of the module pattern\cite[p.~40]{GoodParts}, which means that only a small subset of the code is exposed as globally accessible functions. In order to test the functions in any sensible way, the design typically will have to be improved so that no function has too many responsibilities. This conflicts with the eagerness of most developers to just get something that works without making it more complicated than necessary.

Many \gls{js} developers are used to manually test their \gls{js} in a browser. For someone not experienced with testing, this gives a relatively early feedback loop and although it does not come with the benefits of design, quality and automated testing that \gls{tdd} does, it tends to give a feeling of not doing any extra work and getting the job done as fast as possible. Developers do not want to spend time on mocking dependencies when they are unsure if the solution they have in mind will even work. Once an implementation idea pops up, it can be tempting to just try it out rather than writing tests. If this approach is taken, it may feel like a superfluous task to add tests afterwards since that will typically require some refactoring in order to make the code testable. If the code seems to work good enough, the developer may not be willing to introduce this extra overhead, for good reasons \cite[question~43]{Stenmark}. This tendency is closely related to the concept of spikes in \gls{tdd}, see section~\ref{subsec:spikes}.

As mentioned in section~\ref{subsec:motivation}, tests should be maintainable and test the right thing. Otherwise, responding to changes is harder, and the tests will tend to cause frustration among the developers instead of detecting bugs and driving the understanding and development of the software \cite{Clean}. Key points here are that tests can not substitute careful decision-making and that you also need to think of the maintainability of the tests themselves, they suffer from code rot just like production code so they need to be treated similarly \cite[question~36]{Ahnve}\cite[p.~123-133]{Clean}.

The criteria for maintainability in this context are that the tests should have low complexity (typically short test methods without any control flow), consist of readable code, use informative variable names, have reasonably low level of repeated code (this can be accomplished through using Test Utility Methods \cite[599]{TestPatterns}), be independent of implementations and have meaningful comments (if any). Structuring the code according to a testing pattern such as the Arrange-Act-Assert \cite{C2} and writing the code so that it reads like sentences will help in making the code more readable, in essence by honouring the communicate intent principle \cite[p.~41]{TestPatterns}.

Testing the right thing means focusing on the specifications and behaviour that provides true business value rather than e.g. coverage criteria, and to avoid writing tests for things that does not really matter. Typically some parts of the system will be more complex and require more rigorous testing but there should also be consistency in the level of ambition regarding testing across the entire application. If some part of the code is hard to test it is likely to be beneficial in the long run to refactor the design to provide better testability than to leave the code untested, or else refactoring will be even harder later on \cite{Refactoring}. % Vad är ett dåligt test?

Tests that depend on the \gls{dom} tend to be relatively slow, so it would be impractical to have hundreds of unit tests with \gls{dom} manipulation if you want to be able to run them often \cite[questions~21-22]{Stenmark}. Selenium tests are also known to be slow, the same principle applies there. Only running tests that are connected with changes that have been made is one way to get around this, there is support for this in many testing tools. An alternative is to separate the fast tests from the slow, and configure Autotest\footnote{\url{http://autotest.github.io/}} or some tool based on Guard\footnote{\url{https://github.com/guard/guard/}} such as guard-jasmine\footnote{\url{https://github.com/netzpirat/guard-jasmine/}} to run the fast tests each time a file changes, and only run the slow tests before sending the code to a central source control repository or deploying the application.

\subsection{(JS,ML)Culture, Collaboration and Consensus}
\label{subsec:ccc}

Testing is not about eliminating the risk of bugs or ensuring that every line of code is executed exactly the way it was originally meant to be executed. In order to better understand the main benefits of testing, it is recommended to read books such as Clean Code\cite{Clean}, and to work together with experienced programmers and testers. Without this understanding there is a risk of frustration, inability to decide which tools to use and difficulties in motivating choices related to testing. Collaboration problems may occur when members of a team have different opinions about the purpose of testing. \cite[question~38]{Edelstam}

To get as much benefit as possible from testing, every developer should be involved in the testing effort and preferably the product owner and other stakeholders too. The work of specifying stories can involve agreeing on testing scenarios, thinking about what the desired behaviour is and coming up with examples that can be used as input and expected output in end-to-end tests. \cite[questions~39-40]{Edelstam}\cite[question~30]{Stenmark}

The attitude towards and experience with testing varies heavily, so as a developer you can be forced to not test as much as you would otherwise want to, in order to avoid frustration and dips in productivity by testing other people's code. You should advocate methods that you believe in, especially in the beginning of a new project, but in the end adjusting to the culture is also essential since most benefits of testing appear only after a long term commitment by the whole team. One example is whether everyone uses their own database for testing, this is usually a bad idea but you have to decide whether it is worth the effort to create new databases for yourself, everyone, or switching to another, although these kinds of decisions might be limited by licenses and what the customer wants. \cite[questions~31-32]{Stenmark}\cite[question~36]{Ahnve}

In order to understand the problems with testability that can be seen in \gls{js} applications, it is important to understand how many were first introduced to the language. When jQuery was released in 2006, a coding style evolved based on handlers with \gls{dom} manipulation through selectors and asynchronous callbacks. Since tutorials were written in a quick-and-dirty fashion, even experienced developers failed to apply principles for writing testable and maintainable code. Others lacked the background of software engineering altogether. \cite[question~10]{Stenmark}

An individual concern regarding testing is that it depends on being able to structure the code properly, which in turn tends to rely on experience with multiple programming languages and the ability to make pragmatic decisions, not just encapsulating and applying patterns for the sake of it, without understand why. Otherwise any efforts to produce good code or tests will fail. % Tveksamt on denna rant tillför något
On the other hand, as a beginner it might be better to rely on proven methods until you obtain proper contextual knowledge, rather than trying to get things exactly right from the start. Preferably, beginners should work with experienced developers to avoid risk of reinventing the wheel. \cite[questions~23-24]{Ahnve}


\subsection{Individual Motivation and Concerns}
\label{subsec:motivationconcerns}

What defines successful testing is not which tools are used, the degree of coverage that is achieved or how strictly a certain set of principles or methodologies are followed. Successful testing is defined by how easy it is to make changes, find errors and understand the code. In order to get there, developers must feel that testing is meaningful and that they are allowed to spend time on testing as part of their professional tasks.

Some think that as long as the code works, testing is unnecessary \cite[question~13]{Ahnve}. Perhaps this is true for these people, testing is not for everyone, there are programmers out there that do not test their code but still manage to produce brilliant software. In most cases however, testing leads to better maintainability. The exception being when the tests themselves are not maintainable. \cite[question~33]{Ahnve}\cite[question~28]{Edelstam}

It is important to remember that people tend to care most of things related to their guild, i.e. what they are good at and take pride in. The most embarrassing thing for a seasoned tester might be that a bug has leaked into production, and an art director may be more concerned that everything looks good and care less about the actual functionality. This can be seen in projects where, despite having the same economic risk, completely different approaches to quality control may be taken. In some projects it is ok to release a bug and roll back or update when it is discovered whereas others can spend an entire week performing manual tests, as part of a ``hardening sprint''. Delaying a release by two weeks and having full time developers doing manual testing rather than implementing new functionality may give them a feeling similar to driving extremely slow in a sports car, not to mention the associated cost from a business perspective. This is why releasing and restoring previous versions should be as simple as possible. \cite[question~38]{Ahnve}

\subsection{(ML,JS1)Project risks}
\label{subsec:projectrisks}

Different projects have different fault tolerance. Sometimes a single bug can cause loss of millions of dollars or even lives, whereas in other projects the impact of a failure can be relatively small. The risk with untested \gls{js} is especially high when the code supports business critical operations. For instance, there have been several cases of banks and finance systems being fined for not reporting transactions to the government \cite{Bug1} or giving faulty advice to investors \cite{Bug2} due to application failure. A web-shop may lose orders and any website that is perceived as broken can harm trademarks associated with it and change people's attitudes for the worse.

The connection between how much testing is carried out and how much risk is involved with a project is not as strong as one might think. The company culture and the background of the developers have great impact too, especially when it comes to economic risks. If the stakeholders are not used to testing and the developers are inexperienced with the tools of the trade, chances are that they will rely on manual testing or no testing at all, whereas utilising \gls{ci} with good rollback capabilities can lead to qualitative and cost effective delivery without any manual testing. On the other hand, no company would rely on just \gls{ci} if the application is safety critical, for example regulating nuclear coolant or flight navigation. In such scenarios there should be both automatic tests, rigorous manual testing routines and frequent validation reviews. \cite[questions~14,~38]{Ahnve}

\subsection{(ML,JS)Frameworks}
\label{subsec:frameworks}

Programming languages in use today typically have frameworks to help with standard structure and other generic problems. \gls{js} is certainly no exception, the number of web application frameworks that focus on \gls{js} has increased a lot during the last few years alone. When application frameworks such as Backbone first appeared, they revolutionised scalability and structure of \gls{js} applications \cite[question~23]{Ahnve}\cite[question~11]{Rovegard}. The advent of application frameworks also gave new support for testing, for instance, the AngularJS framework uses Jasmine and Karma in the official tutorial.

\begin{quote}
``Since testing is such a critical part of software development, we make it easy to create tests in Angular so that developers are encouraged to write them'' \cite{AngularTemplates}
\end{quote}

This likely contributes to increasing the testing of \gls{js} code by developers using such frameworks. This effect is accomplished both through tutorials and the way application frameworks are designed. For instance, the moustache templates of CanJS can be used to extract code from views, where it is easier to test and reduces view logic complexity \cite[question~32]{Ahnve}.

There is a tendency of testing frameworks being tied to application frameworks and vice versa -- Karma and protractor are for instance rather AngularJS specific, and angular-mocks is designed to work with Jasmine \cite[question~7]{Rovegard}. Several of the interviewees had experienced that some application frameworks are uncompromising when it comes to using only a subset of the framework or using another testing framework than what the application framework was designed for, although there are certainly exceptions to this rule as well, such as CanJS \cite[question~22]{Ahnve}\cite[question~41]{Rovegard}.

There are a number of \gls{bdd} frameworks available that are meant to bring testing closer to the specification and story work, as proposed in section~\ref{subsec:ccc}. They aim to mimic the way stories are written to create a ubiquitous natural language that can be understood by both programmers and non-programmers. The way this is done is by building sentences with a Given, Then, When (GTW) form. Cucumber\footnote{\url{http://cukes.info/}} is a popular framework that was designed for this in the Ruby programming language and there is a \gls{js} version called Cucumber.js\footnote{\url{https://github.com/cucumber/cucumber-js}}.
Yadda,\footnote{\url{https://github.com/acuminous/yadda}}
Kyuri,\footnote{\url{https://github.com/nodejitsu/kyuri}}
Cucumis\footnote{\url{https://github.com/noblesamurai/cucumis}} and
Jasmine-given\footnote{\url{https://github.com/searls/jasmine-given}}
are other examples of \gls{js} GTW-frameworks. \cite[section 8.4]{BDDJS}

Since different assertion frameworks have different syntax, they influence the readability of tests. This is presumably one of the main reasons why \gls{bdd} frameworks such as Jasmine and Buster have become popular, despite lack of certain features such as \texttt{beforeAll} (which would be useful to construct \gls{testfixture} setups) or sophisticated ways of selecting which specs to run based on more than just file names. There seems to be varying developer cultures behind these frameworks, that determine whether or not contributions from the community is allowed into the code base. \cite[question~7]{Ahnve}\cite[question~5-6]{Rovegard}

Apart from testing frameworks, your ability to write tests is also influenced by what application framework you are using \cite[question~8]{Ahnve}. According to Edelstam, if you are able to choose, it is often preferable to have small rather than ``magic bullet'' frameworks for large projects because they tend to be less obtrusive. Exceptions may include when the framework is very well known to the developers or when there is a perfect fit between what you want to achieve and what the framework is designed for, but one should bear in mind that requirements tend to evolve. \cite[questions~48-50]{Edelstam} Stenmark had another view on things and instead recommended small frameworks only for small projects, since the value of bindings and other features tend to outweigh adjustment problems. \cite[questions~12-14]{Stenmark}

Using a suitable application framework and doing test driven development can be mutually beneficial. The framework helps to create structure that simplifies testing and the testing process further improves the quality of the code, allowing you to make better use of the framework and providing incentive for even better structure. \cite[question~15]{Stenmark}

Aside from the increased awareness and knowledge of testing that Node.js has brought to the \gls{js} community from other communities such as Ruby on Rails, the runtime itself facilitates testing since it enables different testing tools to be distributed through npm (Node Package Manager) and run in the terminal. Previously, you had to load a HTML page in a browser in order to run your tests. \cite[question~9]{Stenmark}


\section{(JS,ML)Conclusions}
\label{sec:conclusions}

This section contains a summary of the main points of this thesis, a discussion about what could have been done differently, and suggestions for the future.

\subsection{(JS,ML)Lessons learned}

Both the interviews, the literature studies and the practical work indicated that one of the most crucial things for successful testing is finding suitable abstraction levels. If tests contain too much details they will be harder to read, and the same applies to functions in the production code, except that will also impact testability. It is therefore useful to introduce domain-specific language in tests \cite[p.~127]{Clean} and thereby encourage proper modularisation in the production code.

JS is used differently, it is not just another programming language. Because it is implemented in practically every browser, it is used by almost all web developers, regardless of their experience with programming. Frameworks such as jQuery also influence how people write their JavaScript code, often in a way that is negative to testing. On the other hand, other frameworks have the opposite effect and there is nothing inherently bad with using a powerful tool such as jQuery, you just need to be careful to separate code that does different things. If you don't know how the code works, as can be the case when copying code from an external source, then you can not do this. You also need to be aware of good practices.

The web is a difficult environment for testing because of many dependencies and events. This makes it harder to test, but also more important.

Striving for full coverage is not compatible with \gls{bdd} and not being implementation specific. As a novice tester, it can be hard to find a good balance between writing many tests and writing good tests.

In a project without any testing routines, you may save time initially but there is no way of confirming that everything works as it should. Without automated tests or testing protocols, bugs will be harder to reproduce. Testing routines in the form of test plans can be hard to keep up to date and execute regularly since the testing activity depends on someone actually taking the time to do it. If something goes wrong you may have to repeat the procedure to ensure that it was not just an error in the testing. An absence of unit tests can lead to exorbitant amount of debugging in order to find the source of an error.

When automatic regression tests are missing, making changes to the code is error prone. Issues related to browser compatibility or subtle dependencies between functions and events are easier to overlook in the absence of tests, and a proper toolset is required to test multiple platforms.

Many people have never set up a testing framework in a project and overestimate how hard it is. Automatic test generation or integration with monitoring services can be hard, but installing a test framework and write the first test is typically not. However, it is worth giving careful thought on what combination of tools to use, to enable a pleasant workflow and ensuring that the tests can be written in a desirable style.

\subsection{Insights from interviews}

It seems that in the last couple of years, the number of people testing their \gls{js} has increased significantly\cite[question~1]{Edelstam}. This has been observed through asking people that do it to raise their hands during tech talks, two years ago only a few raised their hands when asked such a question whereas now almost every single one does it.

According to Edelstam, a likely reason for this change is that the tools have become better and that there are more examples of how to do it. This has caused the opinion that \gls{js} is too user interface centred to recede, as people have realised how it can be done. Few people were ever against \gls{tdd} or testing in general, much thanks to positive experiences from testing in Ruby on Rails projects, which actually act as great examples of that testing wide ranges of interfaces is possible and that many feel that it is necessary. Perhaps the most common reason for not testing is that the code has not been written in a testable fashion. \cite[questions~2-3]{Edelstam}

A common experience when writing tests is that you put a lot of effort into the tests and do not write that much production code in comparison, but that can be a good thing! Because then you spend more time thinking about the problem and possible abstractions, which tends to lead to elegant solutions. If you write the tests before the code, you will run into the same problems as you would have done if you wrote the code first, the difference is that you get to think about the design rather than staring at incomplete code when solving the problem. \cite[question~8]{Edelstam}

The feeling of being limited and not productive when writing tests can stem from a badly chosen level of ambition or that the focus of the tests is wrong, which in turn can be based on poor understanding of what tests are good for. Coding without tests can be much like multitasking, you get an illusion of being more productive than you actually are. One of the positive effects of \gls{tdd} is that it can prevent you from losing track of direction, and helps you in making clear delimitations, since trying to be smart by allowing a function to do more than one thing means more tests. Testing will not automatically provide you with good ideas regarding where you are heading, but once you have gotten such an idea, testing tends to be easier and help you discover new aspects and scenarios which might would have been left unnoticed without tests. \cite[question~8]{Edelstam}

When deciding what to test, it pays off to focus on parts that are central to how the application is perceived, for instance pure logic and calculations might be more important than pictures and graphs. An error that propagates through the entire application is more serious than if a single picture is not displayed properly. If a test turns out to be difficult to write or frequently needs to be replaced by another test, it is usually worth considering not testing that part at all. \cite[questions~9-10]{Edelstam}

In June this year, Kent Beck wrote the following tweet\footnote{A tweet is a message sent to many using the social media \url{www.twitter.com}}:

\begin{quote}
``that was tough--it almost never takes me 6 hours to write one test. complicated domain required extensive research.''
\end{quote}

One of the responses was that many would have given up on writing that test. Beck replied that if you don't know how to write the test, you don't know how to write the code either \cite{TwitterKentBeck}. What many probably fail to realise about why testing can be time consuming and hard, is that when writing tests you encounter problems that you would have to solve anyway. The difference is that you solve the problems by formulating tests rather than staring at production code for the same amount of time. \cite[question~11]{Edelstam}

Tools are an important part of facilitating testing, so that people are not so deterred by the initial effort required to get started. Yeoman is one example of a framework that can help you in quickly getting started with a project that is structured so that testing becomes easier. For already existing projects, the increased maturity of tools such as PhantomJS and Mocha is also truly helpful. \cite[questions~11-12 and 20]{Edelstam}

Error reports are useful feedback that tests provide. The quality of these reports vary depending on which testing frameworks you are using and how you write your tests. When using PhantomJS to run tests, some test failures require you to run the tests in a browser in order to get good error reports. \cite[question~12]{Edelstam}

An important difference between using a headless browser such as PhantomJS to run your tests compared to JsTestDriver, or other drivers that allow you to test in several browsers, is that a headless browser provides no information about how your code performs on different \gls{js} implementations. Reasons why you might still decide to do so include speed, easier integration with build tools such as Jenkins, and that it yields the same results as long as the tests focus on logic rather than compatibility. \cite[questions~13-15]{Edelstam}

% TODO: section: Ways of testing, selenium vs unit test in browser. headless vs real browser.

One could argue that \gls{js} is better suited for testing than most other programming languages because of the built in features than often make stubbing frameworks redundant. The object literals can be used to define fake objects and it is easy to manually replace a function with another and then restore it. An advantage with using manually written fakes is that it tends to make the code easier to understand since knowledge about specific frameworks or DSLs (Domain Specific Language) is not required. \cite[questions~20-21]{Edelstam}\label{LiteralFakes}

The problems that some people experience with testing front end interfaces sometimes have to do with poor modularity. For instance, the presentation layer should not be responsible for calling large APIs. Tools such as RequireJS can be used to handle dependencies but if each part of the application has too many or large dependencies, mocking becomes a daunting task. Typically, these kinds of problems can be solved by introducing new layers of logic and services that separate responsibilities and allows for much cleaner tests. \cite[question~23]{Edelstam}

A common case of user interface testing is form validation. Being test driven does not necessarily mean that you should test that the form exists, has a working submit button and other boilerplate things, unless the form is written in a way that is unique or new to you in some way. A typical approach would rather be to write the code for the form and then add a test that searches for a non-existing validation. The difficult part here is to strike a balance and be alert to when the code is getting so complicated that it is time to start testing, and to avoid writing tests when they are in the way and provide little value. \cite[questions~24-25]{Edelstam}

Being too religious about testing principles leads to conflicts like ``writing tests take too much time from the real job''. If the tests do not provide enough value for them to be worth the effort then they should be written differently or not at all. There is no value in writing tests just for the sake of it. Thinking about architecture and the end product is usually a good thing, because you need awareness of the bigger picture in order to prfioritize correctly and make sure everything will fit together in the end. There is the same risk with tests as with other pieces of code, sometimes you are especially proud or fond of a certain test and unwilling to throw it away. In order to avoid that situation it is often better to think ahead and try things out than to immediately spend time writing tests. \cite[question~27]{Edelstam}

Writing implementation specific tests is a common phenomena that can stem from poor experience of writing tests, extreme testing ambitions or, perhaps most commonly, poor understanding of the system under test. It means that you write the tests based on details about the implementation that you have in mind rather than basing them on what the system should do from an interface point of view. These kind of tests should be avoided since they tend to be hard to understand and usually have to be changed whenever an improvement of the implementation is considered.

A large number of tests is not necessarily a good thing, it is harder to overview and maintain a large collection of tests. Unless the system inevitably is so complicated that extensive testing is justified, it rarely pays off to strive for 100 \% coverage, since it takes too much time and the scenarios that might cause problems are at risk of being overlooked anyway. \cite[question~28]{Edelstam}

Tests can serve to prevent people from breaking code and as examples that help new people understand how an application works and how to continue development \cite[questions~31-32]{Edelstam}. Tests also help to give a clear image of how components fit together and can be a way to concretise a feature or bug.

\subsubsection{(JS,ML0)Why people do not test their JavaScript}
\label{subsubsec:whynotjs}

\Gls{js} has no notion of interfaces. This makes stubbing and mocking harder, reduces the ability to write tests for an interface before there is an implementation and impedes the ability to write modular (testable) code. This is both a reason why testing \gls{js} is hard, and a reason \emph{for} doing it, since testing can compensate for the lack of interfaces by enforcing modularity.

\Gls{js} is often used in the front end layer of applications, an environment with many dependencies and very specific behaviour that can be difficult to handle in terms of code. Traditionally, browser automation tools have been used to cope with these challenges, and the tests often serve multiple purposes such as combining smoke testing with integration testing and system testing, with little or no mocking of dependencies. The main problems are that such tests take long to run, even with the introduction of headless browsers such as PhantomJS, and are typically brittle. Alternatives such as comparing screen shots automatically with diff tools and then manually inspect the ones that differ should probably be considered in many situations, despite the difficulty in making them self-checking. Under certain circumstances it is also possible to break out logic from the front-end code and test it with unit tests.

\subsubsection{(JS0,ML)Why people do not test Standard ML}
\label{subsubsec:whynotsml}

First off, people do test their \gls{sml} code, but not always, and it seems there are no standard ways of doing it. There is a need for a simple but good testing framework, QCheck has great benefits but is too hard to use for the average programmer or in scenarios where the process of writing tests need to be quick. Other frameworks, including DescribeSML, are too immature. The way forward could be to make QCheck easier to use, with new \gls{bdd} syntax options, customisable failure reports and autotest capabilities, or to merge and collaboratively develop emerging testing frameworks further.

\subsection{(JS,ML)Future}

It would be useful to have a fully automated solution for comparing snapshots in a smart fashion. This is possibly an area for image analysis, which could be integrated in testing tools to handle scenarios that are hard today. This has already been done to some extent, see \url{http://www.imagemagick.org/} and \url{http://huddle.github.io/Resemble.js/}, but has not quite reached the industry yet.

As mentioned in section~\ref{subsec:asynchronous}, the issue of testing asynchronous code is not unique to \gls{js}. Researching testing of asynchronous code in other programming languages and applying the same methods to \gls{js} could be an interesting study.

(new books, new attitudes, old problems, tool convergence and revenue/open source)

\subsection{(JS,ML)TODO/ideas}

Översätta själv? Skriva tester på eget språk, domän/kommunicera med produktägare

JavaScript-människor skriver gärna egna ramverk, men vad krävs för att de faktiskt ska användas? (jmf MVC, MVVP, etc, där rätt många faktiskt används, vad krävs för att få samma genomslag)

Ta upp faux-bugg som exempel på problem som en crawler kunde löst, samt diskutera/spekulera kring varför folk tror att det är svårt

Olika drivkrafter/motivation till att skriva tester i JavaScript jämfört med ML - olika användningsområden

Description of work, referera fram i rapporten och använd som rubriker

API: mocka en nivå, kör tester, mocka nästa nivå, kör tester

Löser problemet direkt i webbläsaren, "visuell programmering", test first känns därför bakvänt för gränssnittare, som är problemlösningsorienterade

Teknisk aspekt: greenfield vs. existing code base (testability)

Förklaring av olika typer av tester: Enhetstester bra för att testa algoritmer. Exempel och liknelser: bilar och cykelhjul

Kolla upp coverage-verktyg för JavaScript, och om någon erbjuder rapporter för (cyklomatisk komplexitet)

Ställ frågor, fundera över vilka delar av rapporten som besvarar de frågorna, försök hitta frågor som alla delar är med i att besvara

\printbibliography

\end{document}
